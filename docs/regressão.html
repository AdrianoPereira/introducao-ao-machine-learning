<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 Regressão | Agrupamento</title>
  <meta name="description" content="3 Regressão | Agrupamento" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="3 Regressão | Agrupamento" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 Regressão | Agrupamento" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classificação.html"/>
<link rel="next" href="agrupamento.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />











<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="introdução.html"><a href="introdução.html"><i class="fa fa-check"></i><b>1</b> Introdução</a><ul>
<li class="chapter" data-level="1.1" data-path="introdução.html"><a href="introdução.html#machine-learning"><i class="fa fa-check"></i><b>1.1</b> Machine learning</a></li>
</ul></li>

<li class="chapter" data-level="2" data-path="classificação.html"><a href="classificação.html"><i class="fa fa-check"></i><b>2</b> Classificação</a>
<ul>
<li class="chapter" data-level="2.1" data-path="classificação.html"><a href="classificação.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>2.1</b> <em>k</em>-Nearest Neighbors</a>
<ul>
=======

<li class="chapter" data-level="2.1.1" data-path="classificação.html"><a href="classificação.html#escolha-do-valor-de-k"><i class="fa fa-check"></i><b>2.1.1</b> Escolha do valor de K</a></li>
<li class="chapter" data-level="2.1.2" data-path="classificação.html"><a href="classificação.html#complexidade"><i class="fa fa-check"></i><b>2.1.2</b> Complexidade</a></li>
<li class="chapter" data-level="2.1.3" data-path="classificação.html"><a href="classificação.html#exemplos"><i class="fa fa-check"></i><b>2.1.3</b> Exemplos</a></li>
</ul></li>

<li class="chapter" data-level="2.2" data-path="classificação.html"><a href="classificação.html#árvore-de-decisão"><i class="fa fa-check"></i><b>2.2</b> Árvore de decisão</a>
<ul>

<li class="chapter" data-level="2.2.1" data-path="classificação.html"><a href="classificação.html#conceitos-gerais"><i class="fa fa-check"></i><b>2.2.1</b> Conceitos gerais</a></li>
<li class="chapter" data-level="2.2.2" data-path="classificação.html"><a href="classificação.html#funcionamento"><i class="fa fa-check"></i><b>2.2.2</b> Funcionamento</a></li>
<li class="chapter" data-level="2.2.3" data-path="classificação.html"><a href="classificação.html#problemas-com-overfitting"><i class="fa fa-check"></i><b>2.2.3</b> Problemas com overfitting</a></li>
<li class="chapter" data-level="2.2.4" data-path="classificação.html"><a href="classificação.html#exemplos-1"><i class="fa fa-check"></i><b>2.2.4</b> Exemplos</a></li>
</ul></li>
</ul></li>

<li class="chapter" data-level="3" data-path="regressão.html"><a href="regressão.html"><i class="fa fa-check"></i><b>3</b> Regressão</a>
<ul>
<li class="chapter" data-level="3.1" data-path="regressão.html"><a href="regressão.html#regressão-linear"><i class="fa fa-check"></i><b>3.1</b> Regressão Linear</a>
<ul>

<li class="chapter" data-level="3.1.1" data-path="regressão.html"><a href="regressão.html#coeficientes-da-regressão-linear"><i class="fa fa-check"></i><b>3.1.1</b> Coeficientes da regressão linear</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="regressão.html"><a href="regressão.html#máquinas-de-vetores-de-suporte"><i class="fa fa-check"></i><b>3.2</b> Máquinas de vetores de suporte</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="regressão.html"><a href="regressão.html#kernels"><i class="fa fa-check"></i><b>3.2.1</b> <em>Kernels</em></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="agrupamento.html"><a href="agrupamento.html"><i class="fa fa-check"></i><b>4</b> Agrupamento</a><ul>
<li class="chapter" data-level="4.1" data-path="agrupamento.html"><a href="agrupamento.html#o-que-é-um-agrupamento"><i class="fa fa-check"></i><b>4.1</b> O que é um agrupamento?</a></li>
<li class="chapter" data-level="4.2" data-path="agrupamento.html"><a href="agrupamento.html#técnicas-de-agrupamento"><i class="fa fa-check"></i><b>4.2</b> Técnicas de agrupamento</a><ul>
<li class="chapter" data-level="4.2.1" data-path="agrupamento.html"><a href="agrupamento.html#técnicas-baseadas-em-partição"><i class="fa fa-check"></i><b>4.2.1</b> Técnicas baseadas em Partição</a></li>
<li class="chapter" data-level="4.2.2" data-path="agrupamento.html"><a href="agrupamento.html#técnicas-baseadas-em-hierarquia"><i class="fa fa-check"></i><b>4.2.2</b> Técnicas baseadas em Hierarquia</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="agrupamento.html"><a href="agrupamento.html#kmeans"><i class="fa fa-check"></i><b>4.3</b> Kmeans</a><ul>
<li class="chapter" data-level="4.3.1" data-path="agrupamento.html"><a href="agrupamento.html#como-avaliar-o-kmeans"><i class="fa fa-check"></i><b>4.3.1</b> Como avaliar o Kmeans?</a></li>
<li class="chapter" data-level="4.3.2" data-path="agrupamento.html"><a href="agrupamento.html#como-definir-a-quantidade-de-grupos"><i class="fa fa-check"></i><b>4.3.2</b> Como definir a quantidade de grupos?</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="agrupamento.html"><a href="agrupamento.html#agrupamento-hierarquico---método-aglomerativo"><i class="fa fa-check"></i><b>4.4</b> Agrupamento Hierarquico - Método Aglomerativo</a><ul>
<li class="chapter" data-level="4.4.1" data-path="agrupamento.html"><a href="agrupamento.html#qual-método-de-ligação-deve-ser-usado"><i class="fa fa-check"></i><b>4.4.1</b> Qual método de ligação deve ser usado?</a></li>
<li class="chapter" data-level="4.4.2" data-path="agrupamento.html"><a href="agrupamento.html#como-definir-a-quantidade-de-grupos-1"><i class="fa fa-check"></i><b>4.4.2</b> Como definir a quantidade de grupos?</a></li>
<li class="chapter" data-level="4.4.3" data-path="classificação.html"><a href="classificação.html#exemplos"><i class="fa fa-check"></i><b>4.4.3</b> Exemplos</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="agrupamento.html"><a href="agrupamento.html#saiba-mais"><i class="fa fa-check"></i><b>4.5</b> Saiba mais</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="considerações-finais.html"><a href="considerações-finais.html"><i class="fa fa-check"></i><b>5</b> Considerações finais</a></li>
<li class="chapter" data-level="6" data-path="referências-bibliográficas.html"><a href="referências-bibliográficas.html"><i class="fa fa-check"></i><b>6</b> Referências Bibliográficas</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Agrupamento</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regressão" class="section level1">
<h1><span class="header-section-number">3</span> Regressão</h1>
<p>Todas as pessoas pelo menos uma vez na vida já sentiu ou sentirá a necessidade
de prever algum acontecimento futuro. Estamos a todo momento assimilando
informações para realizar alguma tomada de decisão, seja de forma intrínseca ou
não. No contexto de Machine Learning (ML) isso é feito pela técnicas de
regressão. A regressão é uma ferramenta que busca modelar relações entre
variáveis dependentes e independentes através de métodos estatísticos
<span class="citation">(Soto 2013)</span>.</p>
<p>Uma variável independente, normalmente representada pela variável <span class="math inline">\(x\)</span>,
caracteriza uma grandeza que está sendo manipulada durante um experimento e que
não sofre influência de outras variáveis. Já a variável dependente, normalmente
representada pela variável <span class="math inline">\(y\)</span>, caracteriza valores que estão diretamente
associados à variável independente, ou seja, ao ser manipulada os valores
variável independente, o valor das variáveis dependentes também sofrem
alterações. Na Figura <a href="regressão.html#fig:happinessWorld">3.1</a> é apresentada a relação entre a
expectativa de vida baseada e um índice de felicidade calculado em diversos
países obtidos a partir de um levantamento feito por <span class="citation">Helliwell et al. (2020)</span>. A
variável independente nesse exemplo é representada pelo índice de felicidade e a
expectativa de vida age como variável independente, dessa forma pode ser
observada uma tendência de expectativa de vida maior em países com alto índice
de felicidade, com uma força de correlação de 0,77.</p>
<div class="figure" style="text-align: center"><span id="fig:happinessWorld"></span>
<img src="assets/happiness_world.png" alt="Relação entre o índice de felicidade e expectativa de vida. Fonte: [@helliwell2020social]"  />
<p class="caption">
Figure 3.1: Relação entre o índice de felicidade e expectativa de vida. Fonte: <span class="citation">(Helliwell et al. 2020)</span>
</p>
</div>
<p>As relações entre as variáveis dependentes e independetes são feitas através de
algum coeficiente de correlação. Uma das métricas de correlação mais utilizadas
é o coeficiente de Pearson, que mede a associação linear entre duas variáveis
<span class="citation">(Kirch 2008)</span>. Esse coeficiente de correlação pode ser definido pela
Equação <a href="regressão.html#eq:corr-pearson">(3.1)</a>, onde <span class="math inline">\(n\)</span> é o total de amostras, <span class="math inline">\(\overline{x}\)</span>
e <span class="math inline">\(\overline{y}\)</span> são as médias aritméticas de ambas as variáveis. Os valores do
coeficiente de Pearson variam entre -1 e 1, de tal forma que quanto mais
próximos desses extremos, melhor correlacionado estão as variáveis. A Figura
<a href="regressão.html#fig:scatterCorrelations">3.2</a> mostra alguns exemplos com gráficos de disperssão
de variáveis com diferentes correlações.</p>
<p><span class="math display" id="eq:corr-pearson">\[\begin{equation} 
    r_{xy} = \frac{{}\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}
    {\sqrt{\sum_{i=1}^{n} (x_i - \overline{x})^2(y_i - \overline{y})^2}}
\tag{3.1}
\end{equation}\]</span></p>
<div class="figure" style="text-align: center"><span id="fig:scatterCorrelations"></span>
<img src="assets/correlations.png" alt="Diferentes correlações entre variáveis. Fonte: [@helliwell2020social]"  />
<p class="caption">
Figure 3.2: Diferentes correlações entre variáveis. Fonte: <span class="citation">(Helliwell et al. 2020)</span>
</p>
</div>
<p>Os métodos de regressão se utilizam dessas correlações entre as variáveis para
estimar valores não existentes na amostra ou conjunto de dados. Entretanto, nem
sempre essas correlações são tão explicítas assim, sendo necessário outras
abordagens mais robustas para realizar as previsões.Em ML os modelos de
regressão podem ser criados a partir de diversas abordagens, desde as mais
simples com poucas configurações de parâmetros e de fácil interpretação do
funcionamento, até as abordagens mais complexas. Os métodos de regressão
abordados neste capítulo serão <code>Regressão linear</code>,
<code>Máquina de vetores de suporte</code> e <code>Árvores de decisão</code>.</p>
<div id="regressão-linear" class="section level2">
<h2><span class="header-section-number">3.1</span> Regressão Linear</h2>
<p>A regressão linear é um dos métodos mais intuitivos e utilizados para essa
finalidade. Esses métodos são dividos em dois grupos, a regressão linear simples
(RLS) e regressão linear múltipla (RLM). A RLS tem como objetivo estabelecer uma
relação entre duas variáveis através de uma função, que pode ser definida por:</p>
<p><span class="math display" id="eq:rls-function">\[\begin{equation} 
    y_{i} = \alpha+\beta x_{i}
\tag{3.2}
\end{equation}\]</span></p>
<p>Onde <span class="math inline">\(y_{i}\)</span> é a variável alvo, <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(beta x_{i}\)</span> são coeficientes
calculados pela regressão, que representam o intercepto no eixo <span class="math inline">\(y\)</span> e inclinação
da reta, respectivamente.</p>
<p>A RLM é semelhante semelhante à RLS, porém possui multiplas variáveis
preditoras, e pode ser definida por:</p>
<p><span class="math display" id="eq:rlm-function">\[\begin{equation} 
    y_{i} = \alpha+\beta x_{i1}+\beta x_{i2}+...+\beta x_{in}
\tag{3.3}
\end{equation}\]</span></p>
<p>Onde <span class="math inline">\(y_{i}\)</span> é a variável alvo, <span class="math inline">\(\alpha\)</span> continua sendo o coeficiente de
intercepto e <span class="math inline">\(\beta x_{ip}\)</span> o é coeficiente angular da <span class="math inline">\(p\)</span>-ésima variável. Ambos
os métodos podem ainda serem somados a um termo <span class="math inline">\(\epsilon\)</span> de erro.</p>
<div id="coeficientes-da-regressão-linear" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Coeficientes da regressão linear</h3>
<p>Existem diversas abordagens para se calcular os coeficientes <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span>
da equação da regressão linear, as técnicas baseadas em mínimos quadrados
oridinários e gradiente descendente são as mais comuns. A seguir serão
apresentados os funcionamentos dessas abordagens.</p>
<div id="métodos-dos-quadrados-ordinários" class="section level4">
<h4><span class="header-section-number">3.1.1.1</span> Métodos dos quadrados ordinários</h4>
<p>O Método dos quadrados ordinários (MQO) ou método dos mínimos quadrados (MMQ),
busca encontrar o melhor valor para os coeficientes citados anteriormente, de
tal forma que a diferença absoluta entre o valor real e o predito pela função
seja a menor possível entre todos os pontos. A Figura <a href="regressão.html#fig:ols">3.3</a> mostra um
exmplo de regressão linar utilizando o MQO para o conjunto pontos descritos na
tabela a seguir:</p>
<p>| Variável independente | Variável dependente |
| 0.44 | 5.52 |
| 1.74 | 8.89 |
| 0.41 | 4.05 |
| 1.84 | 9.31 |
| 0.98 | 6.57 |
| 1.22 | 8.27 |
| 1.53 | 6.93 |
| 1.04 | 6.41 |
| 0.59 | 6.93 |
| 0.38 | 6.98 |</p>
<div class="figure" style="text-align: center"><span id="fig:ols"></span>
<img src="assets/ols.png" alt="Exemplo do método dos quadrados ordinários."  />
<p class="caption">
Figure 3.3: Exemplo do método dos quadrados ordinários.
</p>
</div>
<p>Para se chegar no resultado apresentado na Figura <a href="regressão.html#fig:ols">3.3</a>, os
coeficientes da regressão linear foram ajustados utilizando derivadas parciais,
de tal tal forma que o erro quadrático médio entre entre a função e cada um dos
pontos fossem a menor possível. A Figura <a href="regressão.html#fig:ols-steps">3.4</a> mostra o ajuste
dos coeficientes da equação em relação a cada ponto.</p>
<div class="figure" style="text-align: center"><span id="fig:ols-steps"></span>
<img src="assets/ols-steps.png" alt="Ajuste da regressão linear por método dos quadrados ordinários."  />
<p class="caption">
Figure 3.4: Ajuste da regressão linear por método dos quadrados ordinários.
</p>
</div>
</div>
<div id="gradiente-descendente" class="section level4">
<h4><span class="header-section-number">3.1.1.2</span> Gradiente descendente</h4>
<p>O método do gradiente descendente (GD) é uma das técnicas mais utilizadas para
otimização de modelos de ML. Este é um método interativo que busca encontrar os
coeficiente <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span> através da minimização de uma função de custo,
que normalmente é o erro quadrático médio (MSE - sigla do inglês,
<em>mean squared error</em>).</p>
<p>O GD funciona de forma iterativa e inicializa os coeficientes com um valor
predefinido ou aleatório. Em cada iteração é obtido o somatório do erro entre
todos os valores das variáveis dependentes e valores calculados pela função. Com
base nesse erro e em uma taxa de aprendizagem do modelo predefinida, os valores
dos coeficientes da função são atualizados para a próxima iteração. A taxa de
aprendizagem deve ser definda com um valor equilibrado. A definição de um valor
muito alto para a taxa de aprendizagem pode levar o modelo a cair em um mínimo
local, ou seja, o modelo não consegue chegar em seu melhor ajuste. Já quando a
taxa de aprendizagem é definida com um valor muito baixo, o modelo demora mais
tempo para chegar no ajuste ideal, necessitando de muito mais tempo e
processamento até que haja a convergência. A Figura <a href="regressão.html#fig:learning-rate">3.5</a>
mostra o comportamento do GD com diferentes categorias de valores mencionadas
para a taxa de aprendizagem.</p>
<div class="figure" style="text-align: center"><span id="fig:learning-rate"></span>
<img src="assets/learning-rate-gd.png" alt="Problemas na taxa de aprendizado do gradiente descendente."  />
<p class="caption">
Figure 3.5: Problemas na taxa de aprendizado do gradiente descendente.
</p>
</div>
<p>Os prinicipais parâmetros a serem definidos nessa abordagem são a taxa de
aprendizagem e o número de iterações. Considerando os pontos utilizados no
exemplo anterior, foi aplicada a regressão linear utilizando o GD como método de
atualização dos coeficientes. A Figura <a href="regressão.html#fig:gd-example1">3.6</a> mostra o ajuste da
função, custo e os coeficientes <span class="math inline">\(\alpha\)</span> e <span class="math inline">\(\beta\)</span> ao longo de 50 iterações com
taxa de aprendizado muito baixa. Nessa figura pode ser observado que as
iterações finalizam antes da convergência do modelo.</p>
<div class="figure" style="text-align: center"><span id="fig:gd-example1"></span>
<img src="assets/gradient-descendent-small.png" alt="Regressão linear com taxa de aprendizagem baixa no gradiente descendente."  />
<p class="caption">
Figure 3.6: Regressão linear com taxa de aprendizagem baixa no gradiente descendente.
</p>
</div>
<p>Como mencionado anteriormente, uma taxa de aprendizagem muito grande também
interfere no ajuste dos coeficientes, uma vez o modelo não consegue atingir o
mínimo global. A Figura <a href="regressão.html#fig:gd-example2">3.7</a> mostra o resultado da execução da
regressão linear utilizando uma taxa de aprendizagem muito grande no GD.</p>
<div class="figure" style="text-align: center"><span id="fig:gd-example2"></span>
<img src="assets/gradient-descendent-large.png" alt="Regressão linear com taxa de aprendizagem alta no gradiente descendente."  />
<p class="caption">
Figure 3.7: Regressão linear com taxa de aprendizagem alta no gradiente descendente.
</p>
</div>
<p>Já com uma taxa de aprendizagem equilibrada, o GD é capaz de ajustar os
coeficientes de forma mais eficiente. A Figura <a href="regressão.html#fig:gd-example3">3.8</a> mostra
o resultado do algoritmo executado com uma taxa de aprendizagem mais
equilibrada. Como os valores iniciais dos coeficientes são definidos de forma
aleatória, nas primeiras iterações o gradiente apresenta uma alta pertubação,
que vai se atenuando ao longo das épocas.</p>
<div class="figure" style="text-align: center"><span id="fig:gd-example3"></span>
<img src="assets/gradient-descendent.png" alt="Regressão linear com taxa de aprendizagem equilibrada no gradiente descendente."  />
<p class="caption">
Figure 3.8: Regressão linear com taxa de aprendizagem equilibrada no gradiente descendente.
</p>
</div>
<p>Para dados com poucas dimensões, ou seja, poucas variáveis preditoras, o MQO é
mais recomendado, pois diferente do GD, não é um algoritmo interativo, e sua
complexidade está associada diretamente à quantidade de pontos. Já o GD tem
melhor performance quando os dados possuem muitas dimensões.</p>
<p>A regressão linear pode ser aplicada em uma vasta variedade de problemas, mas
como foi apresentado ao longo desta seção, é necessário que os dados possuam uma
alta correlação. Este algoritmo está disponível na biblioteca <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html">Scikit-learn</a>
par ser utilizado em Python. Um exemplo utilizando a regressão linear é
apresentado no código a seguir:</p>

<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="regressão.html#cb4-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb4-2"><a href="regressão.html#cb4-2"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="regressão.html#cb4-3"></a></span>
<span id="cb4-4"><a href="regressão.html#cb4-4"></a></span>
<span id="cb4-5"><a href="regressão.html#cb4-5"></a><span class="co"># Definição dos valores de uma única variável preditora</span></span>
<span id="cb4-6"><a href="regressão.html#cb4-6"></a>x <span class="op">=</span> [[<span class="dv">0</span>], [<span class="dv">1</span>], [<span class="dv">2</span>], [<span class="dv">3</span>]] </span>
<span id="cb4-7"><a href="regressão.html#cb4-7"></a></span>
<span id="cb4-8"><a href="regressão.html#cb4-8"></a><span class="co"># Definição dos valores das variáveis alvo</span></span>
<span id="cb4-9"><a href="regressão.html#cb4-9"></a>y <span class="op">=</span> [<span class="fl">1.3</span>, <span class="fl">2.2</span>, <span class="fl">2.6</span>, <span class="fl">3.3</span>] </span>
<span id="cb4-10"><a href="regressão.html#cb4-10"></a></span>
<span id="cb4-11"><a href="regressão.html#cb4-11"></a><span class="co"># Instanciando regressão linear e ajustando-a aos dados</span></span>
<span id="cb4-12"><a href="regressão.html#cb4-12"></a>lr <span class="op">=</span> LinearRegression().fit(x, y) </span>
<span id="cb4-13"><a href="regressão.html#cb4-13"></a></span>
<span id="cb4-14"><a href="regressão.html#cb4-14"></a><span class="co"># Obtendo os valores aproximados da variável alvo</span></span>
<span id="cb4-15"><a href="regressão.html#cb4-15"></a>y_regression <span class="op">=</span> [lr.predict([xi])[<span class="dv">0</span>] <span class="cf">for</span> xi <span class="kw">in</span> x] </span>
<span id="cb4-16"><a href="regressão.html#cb4-16"></a></span>
<span id="cb4-17"><a href="regressão.html#cb4-17"></a><span class="co"># Criando novos pontos somente com valores da variável preditora</span></span>
<span id="cb4-18"><a href="regressão.html#cb4-18"></a>x_test <span class="op">=</span> [[<span class="op">-</span><span class="dv">1</span>], [<span class="fl">2.75</span>], [<span class="fl">4.3</span>]]</span>
<span id="cb4-19"><a href="regressão.html#cb4-19"></a></span>
<span id="cb4-20"><a href="regressão.html#cb4-20"></a><span class="co"># Aplicando regressão linear para prever os valores da variável alvo</span></span>
<span id="cb4-21"><a href="regressão.html#cb4-21"></a>y_test <span class="op">=</span> lr.predict(x_test)</span>
<span id="cb4-22"><a href="regressão.html#cb4-22"></a></span>
<span id="cb4-23"><a href="regressão.html#cb4-23"></a><span class="co"># Concatenando dados de treino e teste</span></span>
<span id="cb4-24"><a href="regressão.html#cb4-24"></a>y_regression <span class="op">=</span> [<span class="op">*</span>y_regression, <span class="op">*</span>y_test]</span>
<span id="cb4-25"><a href="regressão.html#cb4-25"></a>x_regression <span class="op">=</span> [<span class="op">*</span>x, <span class="op">*</span>x_test]</span>
<span id="cb4-26"><a href="regressão.html#cb4-26"></a></span>
<span id="cb4-27"><a href="regressão.html#cb4-27"></a><span class="co"># Exibindo dados de treino</span></span>
<span id="cb4-28"><a href="regressão.html#cb4-28"></a>plt.scatter(x, y, color<span class="op">=</span><span class="st">&#39;gray&#39;</span>, label<span class="op">=</span><span class="st">&#39;Dados de treino&#39;</span>) </span>
<span id="cb4-29"><a href="regressão.html#cb4-29"></a></span>
<span id="cb4-30"><a href="regressão.html#cb4-30"></a><span class="co"># Exibindo dados de teste</span></span>
<span id="cb4-31"><a href="regressão.html#cb4-31"></a>plt.scatter(x_test, y_test, color<span class="op">=</span><span class="st">&#39;red&#39;</span>, label<span class="op">=</span><span class="st">&#39;Dados de teste&#39;</span>)</span>
<span id="cb4-32"><a href="regressão.html#cb4-32"></a></span>
<span id="cb4-33"><a href="regressão.html#cb4-33"></a><span class="co"># Exibindo reta da regressão linear</span></span>
<span id="cb4-34"><a href="regressão.html#cb4-34"></a>plt.plot(x_regression, y_regression, color<span class="op">=</span><span class="st">&#39;black&#39;</span>, label<span class="op">=</span><span class="st">&#39;Regressão linear&#39;</span>)</span>
<span id="cb4-35"><a href="regressão.html#cb4-35"></a></span>
<span id="cb4-36"><a href="regressão.html#cb4-36"></a><span class="co">#Configurações do gráfico</span></span>
<span id="cb4-37"><a href="regressão.html#cb4-37"></a>plt.xlabel(<span class="st">&#39;Variável preditora ($x$)&#39;</span>)</span>
<span id="cb4-38"><a href="regressão.html#cb4-38"></a>plt.ylabel(<span class="st">&#39;Variável alvo ($y$)&#39;</span>)</span>
<span id="cb4-39"><a href="regressão.html#cb4-39"></a>plt.legend()</span>
<span id="cb4-40"><a href="regressão.html#cb4-40"></a>plt.show()</span></code></pre></div>
<p><img src="intro-ao-machine-learning_files/figure-html/unnamed-chunk-4-1.png" /><!-- --></p>
</div>
</div>
</div>
<div id="máquinas-de-vetores-de-suporte" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Máquinas de vetores de suporte</h2>
<p>As máquinas de vetores de suporte (SVM - sigla do inglês,
<em>support vector machine</em>) são modelos de aprendizado de máquina supervisionado
concebido a partir de um conceito inicialmente proposto por <span class="citation">Vapnik and Chervonenkis (1963)</span>.
As SVM podem ser utilizadas tanto para tarefas de classificação, quanto para
tarefas de regressão, sendo uma ótima alteranativa aos modelos de redes neurais
artificiais profundas que exigem um custo computacional muito superior em dados
com muitas dimensões. Outra vantagem na utilização dos modelos baseados em SVM é
que eles não são sensíveis aos <em>outliers</em>, ou seja, valores extremos não causam
ruído no treinamento.</p>
<p>O funcionamento básico das SVM consiste em ajustar a equação de uma reta,
denominada hiperplano de tal forma que a distância entre ela e os pontos com
características diferentes seja maximizada. Um conjunto de <span class="math inline">\(n\)</span> pontos é definido
como <span class="math inline">\((\vec{x_{1}}, y_{1}), (\vec{x_{2}}, y_{2}), ..., (\vec{x_{n}}, y_{n})\)</span>,
onde <span class="math inline">\(\vec{x_{i}}\)</span> são as variáveis independentes representadas por um vetor de
<span class="math inline">\(d\)</span>-dimensões e <span class="math inline">\(y_{i}\)</span> são as variáveis dependentes. A distância maximizada
entre o hiperplano e as fronteiras são definidas como margens e os pontos que
estão no limite dessa margem são os vetores de suporte. Esses componentes podem
ser modelados da seguinte forma:</p>
<p><span class="math display" id="eq:svm-components">\[\begin{equation} 
    \vec{w}\cdot\vec{x}-b =
      \begin{cases}
        &amp; -1, &amp; \text{primeiro vetor de suporte} \\
        &amp; 0, &amp; \text{hiperplano} \\
        &amp; 1, &amp; \text{segundo vetor de suporte}
      \end{cases}
\tag{3.4}
\end{equation}\]</span></p>
<p>Onde <span class="math inline">\(\vec{w}\)</span> é um vetor perpendicular aos pontos, <span class="math inline">\(\vec{x}\)</span> é o vetor do
conjunto de pontos é <span class="math inline">\(b\)</span> é uma constante opcional que pode ser usada como uma
<em>bias</em>. Quando o resultado dessa equação é igual a <span class="math inline">\(1\)</span> ou <span class="math inline">\(-1\)</span> trata-se de um
dos vetores de suporte, quando o resultado é um valor maior que <span class="math inline">\(0\)</span> e menor que
<span class="math inline">\(1\)</span> ou menor que <span class="math inline">\(0\)</span> e maior que <span class="math inline">\(-1\)</span> trata-se de um espaço da margem. A Figura
<a href="regressão.html#fig:linear-svm">3.9</a> mostra um exemplo da aplicação do algoritmo SVM em um
conjunto de dados linearmente separáveis. Nessa figura, o hiperplano é
caracterizado pela linha contínua, os vetores de suporte são as linhas
tracejadas que interceptam os pontos com contorno destacado, e o espaço entre
eles são as margens.</p>
<div class="figure" style="text-align: center"><span id="fig:linear-svm"></span>
<img src="assets/linear-svm.png" alt="SVM para conjunto de dados linearmente separáveis."  />
<p class="caption">
Figure 3.9: SVM para conjunto de dados linearmente separáveis.
</p>

</div>
<p>As primeiras versões das SVM eram limitadas somente para resolução de problemas
linearmente separáveis, como mostrado no exemplo anterior, mas a grande maioria
dos problemas não são linearmente separáveis. Considerando a Figura
<a href="regressão.html#fig:kernels-problem">3.10</a> é muito difícil traçar um hiperplano que separe bem
os pontos de cores diferentes. Uma alternativa para esse problema é aumentar as
dimensões para a representação do hiperplano. Essa tarefa é feita com a
introdução de um conceito definido <em>kernel</em>.</p>
<div class="figure" style="text-align: center"><span id="fig:kernels-problem"></span>
<img src="assets/kernels-problem.png" alt="Conjunto de dados não linearmente separáveis."  />
<p class="caption">
Figure 3.10: Conjunto de dados não linearmente separáveis.
</p>
</div>
<p>Ao traçar um hiperplano não linear com a utilização de <em>kernels</em> é possível
ajustar melhor os vetores de suporte aos dados. A Figura <a href="regressão.html#kernels">3.2.1</a> mostra o
conjunto de dados ajustado com hiperplanos lineares e não lineares.</p>
<div class="figure" style="text-align: center"><span id="fig:kernels"></span>
<img src="assets/kernels.png" alt="Hiperplanos utilizando *kernels* com funções lineares e não lineares."  />
<p class="caption">
Figure 3.11: Hiperplanos utilizando <em>kernels</em> com funções lineares e não lineares.
</p>
</div>
<p>A abordagem utilizando os <em>kernels</em> é uma das principais características desse
modelo de ML, pois faz com que o hiperplano seja ajustado em uma dimensão
superior, utilizando equações de polinômios de maior grau. A Figura
<a href="#kernels-plot"><strong>??</strong></a> mostra graficamente como é realizada essa manipulação.</p>
<div class="figure" style="text-align: center"><span id="fig:kernels-plot"></span>
<img src="assets/kernels.png" alt="Representação gráfica dos dados e da função não linear."  />
<p class="caption">
Figure 3.12: Representação gráfica dos dados e da função não linear.
</p>
</div>
<p>A utilização de <em>kernels</em> é uma das principais características do SVM e faz com
que os modelos baseados nessa abordagem, sejam tão robustos quanto outras
técnicas mais complexas.</p>
<div id="kernels" class="section level3" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> <em>Kernels</em></h3>
<p>A utilização dos <em>kernels</em> em SVM foram introduzidos por <span class="citation">Boser, Guyon, and Vapnik (1992)</span>.
Esses componentes <em>kernels</em> nada mais são do que uma função que serão
responsáveis por maximizar as margens dos vetores de suporte. A maioria das
bibliotecas de ML, já possuem <em>kernels</em> implementados e também permitem a
integração de outras funções customizadas. A lista a seguir aprensenta
brevemente os <em>kernels</em> mais utilizados.</p>
<ul>
<li><strong>Linear: </strong> Como mencionado anteriormente, é eficiente somente para problemas
linearmente separáveis, uma vez que seu ajuste se da através da equação de uma
reta. O <em>kernel</em> linear é definido apenas pelo produto entre duas amostras
<span class="math inline">\(\vec{x_{i}}\)</span> e <span class="math inline">\(\vec{x_{j}}\)</span>:</li>
</ul>
<p><span class="math display" id="eq:linear-kernel">\[\begin{equation} 
    k(\vec{x_{i}}, \vec{x_{j}}) = \vec{x_{i}} \cdot \vec{x_{j}}
\tag{3.5}
\end{equation}\]</span></p>
<ul>
<li><strong>Polinominal: </strong> Os <em>kernels</em> polinomiais popularmente utilizados em tarefas
de processamento de imagens, permitem adicionar curvas aos hiperplanos. Além das
amostras <span class="math inline">\(\vec{x_{i}}\)</span> e <span class="math inline">\(\vec{x_{j}}\)</span>, o <em>kernel</em> polinominal também recebe o
a variável <span class="math inline">\(d\)</span> que indica o seu grau, como definido pela equação:</li>
</ul>
<p><span class="math display" id="eq:polynomial-kernel">\[\begin{equation} 
    k(\vec{x_{i}}, \vec{x_{j}}) = (\vec{x_{i}} \cdot \vec{x_{j}} + 1)^{d}
\tag{3.6}
\end{equation}\]</span></p>
<ul>
<li><strong>Função gaussiana de base radial: </strong> Esse <em>kernel</em> é recomendado quando não
se tem um conhecimento prévio a cerca dos dados. Esse <em>kernel</em> é definido pela
seguinte equação:</li>
</ul>
<p><span class="math display" id="eq:rbf-kernel">\[\begin{equation} 
    k(\vec{x_{i}}, \vec{x_{j}}) = exp \left(-\frac{\lVert\vec{x_{i}} - \vec{x_{j}}\rVert^2}{2\sigma^2} \right)
\tag{3.7}
\end{equation}\]</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classificação.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="agrupamento.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["intro-ao-machine-learning.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
