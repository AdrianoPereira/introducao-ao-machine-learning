[
["introdução.html", "1 Introdução 1.1 Machine learning", " 1 Introdução “As máquinas podem pensar?” A pergunta acima faz parte de um exercício teórico proposto pelo cientista da computação Alan Turing em seu artigo publicado em 1950 (TURING 1950). Conhecido também como jogo da imitação, o teste de Turing constitui, em sua concepção inicial, na interação entre três agentes: um agente interrogador e dois agentes respondentes, onde um dos agentes repondentes é um ser humano e outro uma máquina (computador). A pergunta enviada pelo agente interrogador é recebida por ambos os agentes respondentes, onde cada um deles devem enviar de volta a resposta. Com base nas respostas, o agente interrogador deve determinar quem é o humano é que é a máquina, a partir do momento em que esse agente não consegue mais fazer essa diferenciação, é dito que a máquina passou no teste. A Figura 1.1 mostra o esquema básico desse teste. Figure 1.1: Esquema do teste de Turing clássico. Diversas derivações desse teste surgiram posteriormente, o mais famoso deles e familiar entre a maioria dos internautas é o CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart), mecanismo de seguraça proposto por Von Ahn et al. (2003) para validar requisições através da resolução de pequenos desafios, que podem ser identificação de imagens ou textos distorcidos e com ruídos, e que tem como propósito dificultar o acesso não convencional a formulários, por exemplo, tentar impedir o uso bots. O teste de Turing talvez tenha sido um ponto de partida para o que hoje conhecemos por aprendizado de máquina (ML - sigla do inglês, Machine Learning) . A possibilidade de representar pensamentos em computadores, similares aos dos seres vivos foi um grande marco na humanidade. Atualmente esse conceito está sendo aplicado nas mais diversas áreas, tendo em algumas tarefas, o desempenho superior ao dos seres humanos. O próprio CAPTCHA é um exemplo disso, em algumas de suas versões iniciais o conteúdo ficava tão distorcido, que acabava dificultando a sua identificação pelos humanos, em contrapartida, os algoritmos conseguiam resolver o desafio com certa facilidade. Neste capítulo, será apresentada uma visão geral sobre o Machine Learning, discorrendo sobre as principais classes de algoritmos e aplicações com ênfase na área espacial. Ao final deste capítulo o leitor deverá ser capaz de: Compreender o contexto histórico e a definição do ML; Diferenciar as principais abordagens de treinamento dos modelos de ML; Diferenciar as principais classes de algoritmos de ML; Compreender as etapas mínimas necessárias para a produção de um modelo de ML; 1.1 Machine learning O ML é composto por uma coleção de métodos criados a partir de modelos matemáticos baseados na teoria da estatística que permitem aos computadores automatizarem tarefas com base na descoberta sistemática de padrões nos conjuntos de dados disponíveis ou em experiências passadas (Bhavsar et al. 2017; Alpaydin 2020). Segundo a definição de Samuel (1959), um dos pioneiros do assunto, o aprendizado de máquina é “um campo de estudo que oferece aos computadores a capacidade de aprender sem serem explicitamente programados”. "],
["classificação.html", "2 Classificação 2.1 k-Nearest Neighbors 2.2 Árvore de decisão", " 2 Classificação “Nossas características nos fazem igualmente diferentes” - Autor desconhecido Os algoritmos de Aprendizado de Máquina (AM) supervisionado, apresentados anteriormente, podem ser aplicados para a solução de problemas de Classificação e Regressão. Neste capítulo, serão apresenados os algoritmos de AM para as tarefas de classificação. Para começar, precisamos definir o que é classificação e quais problemas essa pode ser utilizada para resolver. Podemos dizer que classificação é Processo que busca, através de características já conhecidas de uma entidade, determinar qual sua classe (ou categoria). Com isso, ao trabalhar com algoritmos de classificação, no contexto de AM supervisionado, estamos preocupados em utilizar algoritmos que permitam com que características já conhecidas sobre um determinado fenômeno possam ser utilizados para a classificação de novos fenômenos em diferentes contextos de uso e aplicação. Essas características fazem com que os algoritmos de classificação sejam empregados nas mais diversas áreas. (Aggarwal, n.d.) apresenta diversos contextos onde a classificação pode ser utilizada, como a análise de dados biológicos, reconhecimento e filtro de documentos e e-mails, além da possibilidade do reconhecimento e diagnóstico de doenças. Dito isto, é importante considerar que, os algoritmos de AM que são empregados para a realização da classificação podem fazer isto através de diferentes técnicas, o que faz com que exista uma gama de diferentes tipos de algoritmos, que através de técnicas variadas, fazem com que o aprendizado acontece e então que a classificação possa ser aplicada. Neste livro, serão apresentados os algoritmos que aplicam técnicas Probabilísticos, Árvores de decisão e Instance-based Learning. 2.1 k-Nearest Neighbors “Você é definido pelas pessoas com quem anda” - Adaptação de ditado popular Para começar, o primeiro algoritmo que vamos tratar será o k-Nearest Neighbors (kNN), um algoritmo de classificação lazzy learning, que através da análise de vizinhança de amostras de um determinado conjunto de treinamento, define o rótulo de classe das amostras do conjunto de teste. De maneira geral, o que o algoritmo faz é buscar os elementos que estão próximos à amostra que está sendo classificada, e com base nessas amostras que estão próximas faz a classificação. Então, aqui temos uma mistura de conceitos, vamos começar primeiro resumindo o algoritmo em uma frase: Me diga com quem tu andas, que eu digo quem tu és Essa frase ajuda muito a assimilar a ideia geral do algoritmo, mantenha ela em mente durante essa seção. Agora, precisamos entender que o kNN é dividido em duas partes principais: (i) Análise de vizinhança; (ii) Determinação do rótulo da classe. Vamos começar pela primeira parte. Análise de vizinhança; Nesta parte do algoritmo o que se busca é determinar quem são as amostras do conjunto de treinamento que estão mais próximas da amostra de teste que está sendo classificada. Certo, mas nesse caso, o que é “estar próximo” ? De maneira intuitiva, a ideia de proximidade está relacionada a distância, e é exatamente dessa forma que o algoritmo, nesta etapa, faz a determinação dos elementos próximos, para isso, utiliza várias funções de distância. Um exemplo de função de distância é a Distância Euclidiana, sim! a mesma que utilizamos para várias partes de nossas vidas para manipulação de elementos no espaço euclidiano. Você lembra de usar ela para calcular a distância entre dois pontos ? Vamos olhar a fórmula para relembrar Então, a Distância euclidiana é apresentada da seguinte forma \\(\\sqrt{\\sum_{i = 1}^{n} (p_i - q_i)^2}\\) Assim, através dessa função o algoritmo determina os elementos que estão mais próximos de uma determinada amostra. Cabe lembrar que, essa não é a única função, várias outras podem ser utilizadas. Agora, podemos ir para a segunda parte, vamos lá! Determinação do rótulo da classe. Para entender esta segunda parte, devemos nos lembrar que, esse algoritmo é um algoritmo supervisionado, então, quando estamos falando do conjunto amostral de treino, estamos assumindo que esses dados já possuem um rótulo definido e que esses rótulos serão utilizados no processo de classificação dos dados que não tem rótulo. Feito esse lembrete, vamos continuar! Nesta parte do algoritmo, após calcular a distância de cada um dos pontos, ele escolhe os k elementos mais próximos. Com a determinação desses elementos mais próximos, o algoritmo analisa e contabiliza, por classe, quantos são os elementos que compoem a vizinhança, dessa forma, ao final desse processo, o algoritmo sabe quais são as classes vizinhas da amostra a ser classificada e quantos elementos de cada uma dessas classes estão presentes na vizinhança. Feito isso, o algoritmo vai determinar o rótulo da amostra a ser classificada como sendo igual a classe, que na vizinhança possui a maior quantidade de elementos. Somente isso! Viu que interessante, esse é um algoritmo simples e que pode ser muito eficiente dependendo do contexto de uso. Mas espera ai, você pode estar se perguntando, “E a ideia do treino e teste que foi falada antes?”, bem, esse é um algoritmo de classificação lazzy learning lembra ? Ou seja, a parte de treinamento é resumida em apenas organizar ou armazenar os dados, de modo a deixar eles pronto para o uso. Agora, com o objetivo de deixar tudo mais claro, vamos para um exemplo visual, passo a passo da execução do algoritmo! Então, vamos começar considerando que o nosso conjunto amostral é formado por um grupo de pontos de várias cores, onde as cores representam os rótulos de cada um desses pontos e então, a ideia vai ser aplicar o kNN para determinar o rótulo de um novo ponto com base nesse conjunto já existente. Veja aqui que os pontos que já existem representam o conjunto de treino, e o ponto que será classificado, representa o conjunto de teste Os pontos de treinamento são apresentados abaixo (#fig:class_knn1)Espaço euclidiano 1 - Fonte: Produção do autor Beleza! Então, com esse conjunto de dados, vamos adicionar um novo ponto, este é representado pelo ponto vermelho na figura abaixo. Perceba que, este é uma amostra que representa o conjunto de teste, e que, ao ser inserido neste espaço, ele não possui nenhum rótulo definido. (#fig:class_knn2)Espaço euclidiano 2 - Fonte: Produção do autor Certo! Então já temos nosso problema de classificação definido, vamos resolver ele com o kNN. Aqui, o valor de k = 5 (Não se preocupe, vamos voltar nesse valor de k depois), ou seja, na primeira parte do algoritmo, análise de vizinhança, vamos buscar os 5 elementos mais próximos ao ponto vermelho. Feito essas considerações, vamos de maneira manual fazer a aplicação do algoritmo, começando com o primeiro passo, de análise de vizinhança, feito com o cálculo das distâncias. Aqui, você vai perceber que a distância utilizada foi a euclidiana, que apresentamos anteriormente. (#fig:class_knn3)Calculo de distância - Fonte: Produção do autor Com as distâncias calculadas a segunda parte do algoritmo, de determinação do rótulo com base nos vizinhos pode ser iniciada. Para isso, primeiro faz-se a seleção dos 5 elementos mais próximos. (#fig:class_knn4)Seleção dos vizinhos mais próximos - Fonte: Produção do autor Com os vizinhos mais próximos determinados, é feito a contagem desses, separando cada um desses por rótulo, de modo que, a vizinhança é sumarizada em quantidade de elementos por rótulo, como apresentado abaixo. (#fig:class_knn5)Contagem dos vizinhos mais próximos - Fonte: Produção do autor Então, é feita a determinação do rótulo do ponto que está sendo classificado, que como podemos ver, vai receber o rótulo laranja, uma vez que, esta é a classe que mais aparece na vizinhança do ponto vermelho. (#fig:class_knn6)Definição da classe - Fonte: Produção do autor É desta forma que os passos que vimos do algoritmo kNN são materializados frente a um conjunto de dados. Viu que legal! O que o algoritmo faz é exatamente aquilo que está na frase que apresentamos antes. 2.1.1 Escolha do valor de K Legal! Então o algoritmo é deveras muito simples de ser entendido, tem uma quantidade pequena de passos, todos compreensíveis. Mas uma pergunta pode ter surgido durante a explicação do algoritmo, como determinar o valor de k. E bem, para esta pergunta existem várias respostas, aqui, será adotada a abordagem da seleção considerando o melhor resultado, assim, durante o processo de treinamento e definição dos hyperparâmetros do kNN, opta-se pela escolha do valor de k que apresenta o melhor resultado entre os elementos com classe já conhecida e o resultado que o algoritmo está apresentando. Outras abordagens baseadas em otimizações também podem ser uma saída para este caso, mas essas, não serão abordadas neste livro introdutório. 2.1.2 Complexidade Como você pode ter pensando, esse pode não ser um algoritmo computacionalmente muito barato, já que, o cálculo de distância nesse algoritmo que apresentamos, é sempre calculado entre todos os pontos do conjunto de treinamento com o conjunto de teste, o que se pensarmos em apenas poucas quantidades de pontos pode não ser um problema, mas que, com a crescente na quantidade, pode ser inviável. Para isso, considere a Figura abaixo, imagina que o ponto vermelho vai ser classificado, a distância dele para todos os outros pontos terá de ser determinada. (#fig:class_knn7)Níveis de complexidade - Fonte: Produção do autor Como forma de reduzir essa complexidade e a quantidade de elementos que precisam ser contabilizados no cálculo da distância, várias implementações aplicam passos de indexação dos dados, com estruturas de dados como a KD-Tree ou a Quad-Tree, como é o caso do scikit-learn. Essa abordagem de implementação evita com que todos os pontos tenham de ser calculados, possibilitando assim com que apenas os necessários, neste caso, os mais próximos sejam utilizados nos cálculos de distância. 2.1.3 Exemplos 2.2 Árvore de decisão “O importante é não deixar de fazer perguntas” - Albert Einstein A árvore de decisão é um dos algoritmos mais utilizados na área de Aprendizado de Máquina, isso por apresentar bons resultados em diversos contextos e por ser considerado um método transparente, por deixar explícito as regras que estão sendo utilizadas para a tomada das decisões e geração dos resultados. No capítulo anterior, esta classe de algoritmos foi apresentada para a solução dos problemas de regressão, aqui, eles serão postos no contexto de classificação. 2.2.1 Conceitos gerais As árvores de decisão são fundamentalmente formas de representação de conhecimento através de uma estrutura hierarquica de perguntas na forma if-then-else. Isso faz com que a estrutura das árvores de decisão sejam semelhantes a um fluxograma, onde existem nós que são utilizados para representar as perguntas e desses são derivados outros nós, que podem representar a resposta ou mesmo outra pergunta. Essa estrutura é apresentada na Figura @ref(fig:class_dt1). (#fig:class_dt1)Árvore de decisão - Fonte: Produção do autor Para nos familiarizarmos com os conceitos que estão envolvidos nessa representação, vamos olhar com calma cada um dos detalhes presentes na figura. Primeiro, a leitura desse tipo de árvore é feita sempre de cima para baixo já que a raiz da árvore está sempre no topo, como é o caso da Pergunta 1 neste exemplo. Além disso, note também que nessa estrutura existem dois tipos de elementos, as Perguntas e as Respostas, onde, das Perguntas podem sair outras dessas ou mesmo Respostas. As Respostas representam elementos finais e quando aparecem nada pode ser inserido abaixo. Outra coisa importante de ser citada é a característica recursiva das árvores de decisão, nessas, para cada ramo que é seguido após uma pergunta há uma nova árvore, que é criada através das mesmas regras de definição aplicadas na árvore anterior. Antes da aplicação em larga escala dos algoritmos de AM, essas estruturas já eram utilizadas, porém, com a diferença de que toda a sua criação era feita manualmente, através da aplicação do conhecimento vindo de pesquisas e experimentos empíricos. Por exemplo, um banco, antes do Aprendizado de Máquina, ao querer ser mais acertivo nos empréstimos e diminuir a inadimplência, poderia utilizar de seu histórico de empréstimos e criar regras consultáveis que poderiam ser utilizadas como auxílio aos operadores que realizam empréstimos. A representação das regras definidas poderia ser feita através de uma árvore de decisão sem problemas, mas, isso não caracterizada nada de aprendizado de máquina, já que todo o ‘processo de aprendizado’ foi feito manualmente por pessoas. Ao contrário disto, as árvores de decisão no contexto de AM são as responsáveis em olhar para os dados e decidir quais são as perguntas mais adequadas para uma determinada resposta. Isso é feito no algoritmo através de sucessivas divisões no conjunto de dados, de modo que, em cada divisão tem-se novos elementos adicionados na árvore. Para essa ideia ficar clara, vamos começar com um exemplo, neste, há um conjunto de pontos, onde cada cor representa uma classe. A árvore de decisão será treinada com esses dados de modo que novas classificações com base neste treinamento possam ser realizadas. O conjunto de dados e a representação da árvore são feitos na Figura @ref(fig:class_dt2). (#fig:class_dt2)Conjunto de dados 1 - Fonte: Produção do autor Com o conjunto de dados definido, o primeiro passo realizado pela árvore é avaliar quais são as características que melhor definem uma determinada classe. Após fazer isso, a árvore cria uma pergunta que faz com que essa característica identificada como a melhor possa ser utilizada para a divisão do conjunto de dados. Neste caso, a árvore identificou que o conjunto de pontos da classe Vermelha estão majoritariamente nas posições com X acima de 10, então, é criada uma pergunta na árvore que verifica quais elementos são maiores que dez, ao fazer isso a divisão do conjunto de dados é realizada e então novos nós são adicionados na árvore, veja na Figura @ref(fig:class_dt3). (#fig:class_dt3)Conjunto de dados 2 - Fonte: Produção do autor Se lembrarmos a definição feita anteriormente, temos que as árvores de decisão são estruturas recursivas, então, a mesma lógica de busca do elemento que melhor descreve um conjunto de dados e então a divisão é aplicado nos nós resultantes, isso é feito até que não haja mais elementos suficientes para a divisão ou quando em um nó, todos os elementos pertencem a apenas uma classe. Na Figura @ref(fig:class_dt4) uma divisão é feita no nó esquerdo, gerado anteriormente, e no nó esquerdo não é feito mais nada. (#fig:class_dt4)Conjunto de dados 3 - Fonte: Produção do autor Vamos supor agora que a árvore apresentada na Figura (fig:class_dt4) finalizou seu treinamento, de modo que classificações podem ser iniciadas. Você pode se perguntar, mas em alguns nós da árvore há uma mistura de elementos, como ele vai fazer a identificação da classe ? Bem, este é um problema recorrente na aplicação das árvores de decisão, nem sempre, as regras escolhidas vão criar grupos de dados 100% puros (de uma única classe). Mais regras poderia ser adicionadas, mas ai, caimos no problema de overfitting, onde a árvore começa a criar regras específicas para os dados que estão sendo utilizados no treinamento, o que faz seu desempenho ser muito ruim com dados que não sejam do conjunto utilizado no treino. Com relação a definição da classe, o que é feito é definir a classe do nó gerado, considerando a classe mais representativa de cada nó, então, se formos classificar um novo ponto e ele tem X &gt; 10, ele vai ser colocado como classe Vermelha no conjunto de dados. Agora que temos uma visão geral do algoritmo, vamos dar ir adiante a frente e ver como os passos apresentados acima são feitos no algoritmo. 2.2.2 Funcionamento Nesta seção, vamos materializar as ideias que foram apresentadas na seção de Visão Geral, descrevendo os passos envolvidos em cada uma das etapas do treinamento da árvore de decisão que foi apresentado anteriormente. Se lembrarmos dos passos que vimos anteriormente, temos que a criação da árvore ocorre em três etapas Seleção do melhor atributo para a divisão dos registros Utilização deste atributo para a divisão do conjunto de dados, gerando novos nós na árvore Para cada nó gerado no passo 2, aplica, recursivamente o algoritmo de construção, iniciando no passo 1. O algoritmo deve parar a recursão e não dividir mais os nós quando: Todos os dados do nó pertencerem a uma mesma classe; Não há dados suficientes para a divisão; ou Todos os atributos disponíveis nos dados já foram utilizados. Agora, para que seja possível entender os conceitos utilizados na construção da árvore de decisão, vamos passar por cada um dos tópicos listados anteriormente. 2.2.2.1 Seleção do melhor atributo e divisão dos dados Como foi apresentado na visão geral, a primeira etapa do algoritmo de criação de uma árvore de decisão é a seleção do atributo que melhor divide o conjunto de dados. Naquele caso, o algoritmo decidiu fazer o uso do atributo X como base para a divisão, considerando também que o valor deste deveria ser menor que 10. Ambas essas decisões foram tomadas seguindo alguns critérios, que ajudaram a definir as perguntas que melhor dividem os dados. Esses critérios são apresentados nessa seção. Vamos começar com a seleção de atributos. Existem diversas métricas que podem auxiliar o algoritmo de criação da árvore de decisão a escolher os melhores atributos, sendo alguns deles o Índice GINI e o Ganho de informação. Neste documento, vamos considerar o uso da métrica de ganho de informação, neste, as divisões no conjunto de dados são feitas considerando a diminuição da entropia que esta vai causar no conjunto de dados. Calma, eu sei, é muito coisa de uma vez só, mas vamos por partes. Primeiro, a entropia é um conceito utilizado para determinar o grau de desordem do conjunto de dados. Antes de apresentar a fórmula, vamos dar uma olhada em diferentes conjuntos de dados e seus níveis de entropia. (#fig:class_dt5)Níveis de entropia - Fonte: Produção do autor Perceba que, nos conjuntos de dados apresentados na Figura @ref(fig:class_dt5), quando menos mistura (Entenda como confusão neste caso) eu tenho no conjunto de dados, menor é a entropia do sistema. Então, a ideia base do ganho de informação é ir fazendo divisões de modo que os espaços resultantes da divisão tenham a menor entropia possível. Agora que já sabemos o que é, vejamos a fórmula da entropia. \\(E\\:=-\\:\\sum _{i=1}^{^N}\\left(p_i\\cdot log_2\\cdot p_i\\right)\\:\\), onde \\(N\\) = Quantidade total de elementos no conjunto de dados, \\(p_i\\) = Probabilidade de aparição da classe \\(i\\) no conjunto de dados Usando dessas duas informações a entropia do sistema pode ser gerada! Bem, e o Ganho de informação ? Então, ele representa a vantagem que um determinado atributo tem de ser utilizado para a divisão, onde essa vantagem representa o grau de diminuição da entropia do conjunto de dados. O ganho de informação é definido como: \\(GI\\left(Q\\right)=E_0-\\sum _{i=1}^q\\left(\\frac{N_i}{N}E_i\\right)\\:\\), onde \\(E_0\\) = Entropia do nó pai (Nó atual) \\(E_i\\) = Entropia do filho a ser gerado \\(q\\) = Quantidade de nós filho \\(N\\) = Quantidade total de elementos no conjunto de dados \\(N_i\\) = Quantidade de dados a ser inserido no nó filho \\(i\\) Uma vez tendo esses conceitos definidos, cabe deixar claro que, o ganho de informação será o elemento base utilizado no processo de seleção do atributo que melhor divide o conjunto de dados (Que tem maior ganho de informação), e uma vez que este atributo é selecionado, ele passa a ser utilizado para a realização da divisão dos dados. Neste processo de divisão, mais passos podem ser considerados, mas o objetivo deste documento é de apenas apresentar a intuição geral por trás da árvore de decisão e seu processo de treinamento. 2.2.2.2 Exemplo de construção da árvore Para fechar essa parte mais téorica do funcionamento da árvore de decisão e podermos iniciar a parte prática, vamos fazer um exemplo passo a passo da criação de uma árvore de decisão. Neste exemplo, vamos utilizar os dados que estão apresentados na tabela abaixo, esses que foram postos com valores categóricos de maneira a tornar mais simples todo o cálculo passo a passo que será realizado. Está chovendo ? Está com tempo ? Transporte (Classe) Sim Sim Ônibus Sim Não Ônibus Não Sim Caminhada Não Não Ônibus Bom, nesta primeira etapa nossa árvore nem existe, é preciso definir para ela um ponto de partida, um atributo que será utilizada para dividir o conjunto de dados e então ir criando os ramos de decisão. Como vimos na seção anterior, vamos utilizar o Ganho de informação para realizar essa decisão. Então, para começar, vamos calcular o ganho de informação para cada uma dos atributos, afinal, se quisermos saber qual a melhor, precisamos testar todas. Para calcular o ganho de informação, precisamos primeiro calcular as entropias para os nós filhos que serão gerados caso este atributo seja o selecionado para a divisão. Para isso, aplicamos a divisão considerando o atributo e então, para cada nó gerado, calculamos a entropia. Por exemplo, no caso do atributo Está chovendo?, a divisão gerou dois nós. Um para o valor Sim e outro para o Não. Dentro desses dois nós é feita a inserção dos elementos correspondentes. No caso de quando Está chovendo? for Sim, o único transporte possível é o Ônibus, então, ele é o único inserido no nó resultante. O mesmo vale para quando a resposta é Não, mas para esta resposta, são possíveis os transportes Ônibus e Caminhada. A Figura @ref(fig:class_dt6) apresenta a aplicação desta lógica em ambas os atributos (Está chovendo? e Está com tempo?) (#fig:class_dt6)Entropia dos nós filhos - Fonte: Produção do autor Com a entropia contabilizada para cada nó filho, de cada um dos atributos, o ganho de informação pode ser calculado. Vamos então começar calculando a entropia do nó pai, que como visto anteriormente, basicamente representa a entropia da divisão atual dos dados, que neste primeiro momento vai considerar todo o conjunto de dados. Lembre-se de que o calculo é feito considerando o atributo de classe, neste caso Transporte. \\(E_0=-\\left(\\left(\\frac{3}{4}\\cdot log_2\\frac{3}{4}\\right)+\\left(\\frac{1}{4}\\cdot log_2\\frac{1}{4}\\right)\\right)\\:\\:\\approx \\:0.81\\) Beleza! Com isso calculado, vejamos o ganho de informação para cada atributo Está chovendo? \\(GI\\left(EstaChovendo?\\right)\\:=\\:0.81\\:-\\:\\sum _{n=1}^q\\:\\left(\\frac{N_i}{N}E_i\\right)=0.31\\) Está com tempo? \\(GI\\left(EstaComTempo?\\right)\\:=\\:0.81\\:-\\:\\sum _{n=1}^q\\:\\left(\\frac{N_i}{N}E_i\\right)=0.31\\) Opa! Olha que interessante, no final das contas os dois atributos tem a mesma quantidade de ganho de informação, o que indica que, ao selecionar um ou outro, a qualidade da divisão vai acabar sendo a mesma. Neste caso, outros critérios podem ser aplicados para o desempate, mas esses não serão tratados aqui. O ponto para este exercício é que, com os ganhos de informação calculados pode-se fazer a seleção do atributo para iniciar a divisão dos dados e assim começar a construção da árvore de decisão. Bem, é básicamente assim que a árvore de decisão aprende, nos passos seguintes a este, utilizando a propriedade recursiva, citada anteriormente, estes passos vão sendo aplicados até atingir os critérios de parada já apresentados também. Com isso, você sabe agora o processo base que é utilizado para no processo de treinamento de uma árvore de decisão. Uma vez que a árvore é criada, é possível iniciar o processo de classificação dos dados, onde, as instâncias que precisam ser classificadas são colocadas na estrutura da árvore e então a classe é definida pela aplicação das regras, exatamente como vimos no exemplo de visão geral apresentado. Acho que agora você consegue perceber o motivo deste ser um dos métodos de aprendizado de máquina mais utilizados que existe, sua simplicidade de implementação, interpretação junto a seu alto poder de generalização fazem com que este seja aplicado em diversos casos. 2.2.3 Problemas com overfitting Assim como qualquer outro algoritmo, de aprendizado de máquina ou não, as árvores de decisão apresentam alguns problemas, o que não as torna “balas de prata” para todos os contextos. Um dos principais problemas que vem junto às árvores de decisão é o overfitting, neste, a árvore, durante o processo de treinamento, começa a criar regras que consideram características que são muito específicas dos dados que estão sendo utilizados no processo de treinamento, isso faz com que ao ser aplicada em outros dados, que não os de treinamento, problemas com a identificação e classificação acabem surgindo. Então, ao utilizar a árvore de decisão tome cuidado com a quantidade de regras e do tamanho da árvore que está sendo gerado. Isso tudo pode ser controlado durante o processo de implementação, controlando os parâmetros de criação da árvore, como os limites de altura e de quantidade de nós resultados. 2.2.4 Exemplos "],
["regressão.html", "3 Regressão 3.1 Regressão Linear", " 3 Regressão Todas as pessoas pelo menos uma vez na vida já sentiu ou sentirá a necessidade de prever algum acontecimento futuro. Estamos a todo momento assimilando informações para realizar alguma tomada de decisão, seja de forma intrínseca ou não. No contexto de Machine Learning (ML) isso é feito pela técnicas de regressão. A regressão é uma ferramenta que busca modelar relações entre variáveis dependentes e independentes através de métodos estatísticos (Soto 2013). Uma variável independente, normalmente representada pela variável \\(x\\), caracteriza uma grandeza que está sendo manipulada durante um experimento e que não sofre influência de outras variáveis. Já a variável dependente, normalmente representada pela variável \\(y\\), caracteriza valores que estão diretamente associados à variável independente, ou seja, ao ser manipulada os valores variável independente, o valor das variáveis dependentes também sofrem alterações. Na Figura 3.1 é apresentada a relação entre a expectativa de vida baseada e um índice de felicidade calculado em diversos países obtidos a partir de um levantamento feito por Helliwell et al. (2020). A variável independente nesse exemplo é representada pelo índice de felicidade e a expectativa de vida age como variável independente, dessa forma pode ser observada uma tendência de expectativa de vida maior em países com alto índice de felicidade, com uma força de correlação de 0,77. Figure 3.1: Relação entre o índice de felicidade e expectativa de vida. Fonte: (Helliwell et al. 2020) As relações entre as variáveis dependentes e independetes são feitas através de algum coeficiente de correlação. Uma das métricas de correlação mais utilizadas é o coeficiente de Pearson, que mede a associação linear entre duas variáveis (Kirch 2008). Os valores do coeficiente de Pearson variam entre -1 e 1, de tal forma que quanto mais próximos desses extremos, melhor correlacionado estão as variáveis. A Figura 3.2 mostra alguns exemplos com gráficos de disperssão de variáveis com diferentes correlações. Figure 3.2: Diferentes correlações entre variáveis. Fonte: (Helliwell et al. 2020) Os métodos de regressão se utilizam dessas correlações entre as variáveis para estimar valores não existentes na amostra ou conjunto de dados. Entretanto, nem sempre essas correlações são tão explicítas assim, sendo necessário outras abordagens mais robustas para realizar as previsões.Em ML os modelos de regressão podem ser criados a partir de diversas abordagens, desde as mais simples com poucas configurações de parâmetros e de fácil interpretação do funcionamento, até as abordagens mais complexas. Os métodos de regressão abordados neste capítulo serão Regressão linear, Máquina de vetores de suporte e Árvores de decisão. 3.1 Regressão Linear A regressão linear é um dos métodos mais intuitivos e utilizados para essa finalidade. Esses métodos são dividos em dois grupos, a regressão linear simples (RLS) e regressão linear múltipla (RLM). A RLS tem como objetivo estabelecer uma relação entre duas variáveis através de uma função, que pode ser definida por: \\[\\begin{equation} y_{i} = \\alpha+\\beta x_{i} \\tag{3.1} \\end{equation}\\] Onde \\(y_{i}\\) é a variável alvo, \\(\\alpha\\) e \\(beta x_{i}\\) são coeficientes calculados pela regressão, que representam o intercepto no eixo \\(y\\) e inclinação da reta, respectivamente. A RLM é semelhante semelhante à RLS, porém possui multiplas variáveis preditoras, e pode ser definida por: \\[\\begin{equation} y_{i} = \\alpha+\\beta x_{i1}+\\beta x_{i2}+...+\\beta x_{in} \\tag{3.2} \\end{equation}\\] Onde \\(y_{i}\\) é a variável alvo, \\(\\alpha\\) continua sendo o coeficiente de intercepto e \\(\\beta x_{ip}\\) o é coeficiente angular da \\(p\\)-ésima variável. Ambos os métodos podem ainda serem somados a um termo \\(\\epsilon\\) de erro. 3.1.1 Coeficientes da regressão linear Existem diversas abordagens para se calcular os coeficientes \\(\\alpha\\) e \\(\\beta\\) da equação da regressão linear, as técnicas baseadas em mínimos quadrados oridinários e gradiente descendente são as mais comuns. A seguir serão apresentados os funcionamentos dessas abordagens. 3.1.1.1 Métodos dos quadrados ordinários O Método dos quadrados ordinários (MQO) ou método dos mínimos quadrados (MMQ), busca encontrar o melhor valor para os coeficientes citados anteriormente, de tal forma que a diferença absoluta entre o valor real e o predito pela função seja a menor possível entre todos os pontos. A Figura 3.3 mostra um exmplo de regressão linar utilizando o MQO para o conjunto pontos descritos na tabela a seguir: | Variável independente | Variável dependente | | 0.44 | 5.52 | | 1.74 | 8.89 | | 0.41 | 4.05 | | 1.84 | 9.31 | | 0.98 | 6.57 | | 1.22 | 8.27 | | 1.53 | 6.93 | | 1.04 | 6.41 | | 0.59 | 6.93 | | 0.38 | 6.98 | Figure 3.3: Exemplo do método dos quadrados ordinários. Para se chegar no resultado apresentado na Figura 3.3, os coeficientes da regressão linear foram ajustados, de tal tal forma que o erro quadrático médio entre entre a função e cada um dos pontos fossem a menor possível. A Figura 3.4 mostra o ajuste dos coeficientes da equação em relação a cada ponto. Figure 3.4: Ajuste da regressão linear por método dos quadrados ordinários. 3.1.1.2 Gradiente descendente O método do gradiente descendente (GD) é uma das técnicas mais utilizadas para otimização de modelos de ML. Este é um método interativo que busca encontrar os coeficiente \\(\\alpha\\) e \\(\\beta\\) através da minimização de uma função de custo, que normalmente é o erro quadrático médio (MSE - sigla do inglês, mean squared error). O GD funciona de forma iterativa e inicializa os coeficientes com um valor predefinido ou aleatório. Em cada iteração é obtido o somatório do erro entre todos os valores das variáveis dependentes e valores calculados pela função. Com base nesse erro e em uma taxa de aprendizagem do modelo predefinida, os valores dos coeficientes da função são atualizados para a próxima iteração. A taxa de aprendizagem deve ser definda com um valor equilibrado. A definição de um valor muito alto para a taxa de aprendizagem pode levar o modelo a cair em um mínimo local, ou seja, o modelo não consegue chegar em seu melhor ajuste. Já quando a taxa de aprendizagem é definida com um valor muito baixo, o modelo demora mais tempo para chegar no ajuste ideal, necessitando de muito mais tempo e processamento até que haja a convergência. A Figura 3.5 mostra o comportamento do GD com diferentes categorias de valores mencionadas para a taxa de aprendizagem. Figure 3.5: Problemas na taxa de aprendizado do gradiente descendente. Os prinicipais parâmetros a serem definidos nessa abordagem são a taxa de aprendizagem e o número de iterações. Considerando os pontos utilizados no exemplo anterior, foi aplicada a regressão linear utilizando o GD como método de atualização dos coeficientes. A Figura 3.6 mostra o ajuste da função, custo e os coeficientes \\(\\alpha\\) e \\(\\beta\\) ao longo de 50 iterações com taxa de aprendizado muito baixa. Nessa figura pode ser observado que as iterações finalizam antes da convergência do modelo. Figure 3.6: Regressão linear com taxa de aprendizagem baixa no gradiente descendente. Como mencionado anteriormente, uma taxa de aprendizagem muito grande também interfere no ajuste dos coeficientes, uma vez o modelo não consegue atingir o mínimo global. A Figura 3.7 mostra o resultado da execução da regressão linear utilizando uma taxa de aprendizagem muito grande no GD. Figure 3.7: Regressão linear com taxa de aprendizagem baixa no gradiente descendente. Já com uma taxa de aprendizagem equilibrada, o GD é capaz de ajustar os coeficientes de forma mais eficiente. A Figura 3.8 mostra o resultado do algoritmo executado com uma taxa de aprendizagem mais equilibrada. Como os valores iniciais dos coeficientes são definidos de forma aleatória, nas primeiras iterações o gradiente apresenta uma alta pertubação, que vai se atenuando ao longo das épocas. Figure 3.8: Regressão linear com taxa de aprendizagem baixa no gradiente descendente. Para dados com poucas dimensões, ou seja, poucas variáveis preditoras, o MQO é mais recomendado, pois diferente do GD, não é um algoritmo interativo, e sua complexidade está associada diretamente à quantidade de pontos. Já o GD tem melhor performance quando os dados possuem muitas dimensões. A regressão linear pode ser aplicada em uma vasta variedade de problemas, mas como foi apresentado ao longo desta seção, é necessário que os dados possuam uma alta correlação. Este algoritmo está disponível na biblioteca Scikit-learn par ser utilizado em Python. Um exemplo utilizando a regressão linear é apresentado no código a seguir: from sklearn.linear_model import LinearRegression import matplotlib.pyplot as plt # Definição dos valores de uma única variável preditora x = [[0], [1], [2], [3]] # Definição dos valores das variáveis alvo y = [1.3, 2.2, 2.6, 3.3] # Instanciando regressão linear e ajustando-a aos dados lr = LinearRegression().fit(x, y) # Obtendo os valores aproximados da variável alvo y_regression = [lr.predict([xi])[0] for xi in x] # Criando novos pontos somente com valores da variável preditora x_test = [[-1], [2.75], [4.3]] # Aplicando regressão linear para prever os valores da variável alvo y_test = lr.predict(x_test) # Concatenando dados de treino e teste y_regression = [*y_regression, *y_test] x_regression = [*x, *x_test] # Exibindo dados de treino plt.scatter(x, y, color=&#39;gray&#39;, label=&#39;Dados de treino&#39;) # Exibindo dados de teste plt.scatter(x_test, y_test, color=&#39;red&#39;, label=&#39;Dados de teste&#39;) # Exibindo reta da regressão linear plt.plot(x_regression, y_regression, color=&#39;black&#39;, label=&#39;Regressão linear&#39;) #Configurações do gráfico plt.xlabel(&#39;Variável preditora ($x$)&#39;) plt.ylabel(&#39;Variável alvo ($y$)&#39;) plt.legend() plt.show() "],
["agrupamento.html", "4 Agrupamento 4.1 O que é um agrupamento? 4.2 Técnicas de agrupamento 4.3 Kmeans 4.4 Agrupamento Hierarquico - Método Aglomerativo 4.5 Saiba mais", " 4 Agrupamento Afinal, todos nós nos enquadramos em um grupo. - (autor desconhecido) Vimos até agora que os métodos de classificação e regressão usam conjuntos de dados rotulados e usamos tais métodos para encontrarmos modelos que descrevam nossos dados. Em resumo, os métodos de classificação realizam a predição de dados categóricos, de modo oposto, os métodos de regressão fazem a predição de dados contínuos, sendo que ambos precisam dos rótulos para criarem seus modelos. As técnicas de agrupamento operam de modo diferente do que foi visto até então, pois usam conjunto de dados não rotulados, o que modifica à forma com que os métodos aprendem e é isso que vamos ver neste capítulo! Neste capítulo, vamos aprender o que é um agrupamento e suas técnicas, com exemplos práticos usando as linguagens de programação R e Python na plataforma Kaggle. Vamos começar respondendo à pergunta: “O que é um agrupamento?” 4.1 O que é um agrupamento? Para entendermos o conceito de agrupamento, vamos começar com um exemplo prático. Digamos que você tenha um amigo que adora livros, e ele tem muitos deles, mas tem muitas dificuldades em organizá-los. Então, você, uma pessoa muito disposta, sugere ao seu amigo algumas formas de como organizar esses livros, por exemplo: separá-los por gênero, cores ou ordem alfabética. Aderindo às suas dicas, seu amigo decidiu agrupá-los por gênero, assim, com essa nova organização foi possível encontrar diversos grupos de livros separados por gênero, como apresentado na Figura 4.1. Figure 4.1: Antes e depois da organização da prateleira de livros por grupos baseados em gênero. Utilizamos cores para dividir os gêneros na Figura 4.1 apenas para facilitar a vida do design. A partir do exemplo acima, vimos que os grupos foram criados de acordo com uma característica, neste caso, gênero, mas poderíamos ter grupos de livros com mais de uma característica. Por exemplo, grupos com livros do mesmo gênero e o mesmo ano de lançamento, ou do mesmo gênero e com a mesma cor, e assim por diante. Assim, podemos concluir que livros pertencentes ao mesmo grupo são semelhantes entre si, ou seja, possuem características iguais ou parecidas, mas são diferentes se comparados com livros de outros grupos. Com isso, aprendemos um conceito fundamental em agrupamento, que é a definição de um grupo. Em aprendizado de máquina, o ato de separar objetos em grupos (em inglês, clusters) por meio de determinadas características de um conjunto de dados é conhecido como agrupamento (em inglês, clustering), e a maneira com que esses grupos são formados é chamado de técnicas ou métodos de agrupamento. De modo oposto do que vimos nos capítulos anteriores, as técnicas de agrupamento possuem o aprendizado não supervisionado, ou seja, usam conjuntos de dados não rotulados para criarem seus modelos. Diante da vasta quantidade de dados gerados diariamente, provindos de diferentes fontes, como radares, redes sociais e smartphones, torna-se cada vez mais difícil obter conjuntos de dados rotulados, pois, por vezes, é necessária a presença de um especialista para criar ou gerenciar esses rótulos. Desta forma, o uso de técnicas de agrupamento pode auxiliar no processo de descoberta de conhecimento e criação dos rótulos a partir dos grupos gerados. Técnicas de agrupamento caem como luva na mão de cientistas e analista de dados, uma vez que identificam padrões em conjuntos de dados através da organização dos dados em grupos. Em detalhes, os grupos são formados por objetos que possuem a máxima semelhança entre si, e a mínima semelhança com elementos de outros grupos (Aghabozorgi, Shirkhorshidi, and Wah 2015) - como vimos no exemplo dos livros. Para identificar esse grau de semelhança entre os objetos precisamos de uma medida de similaridade, pois é ela que vai nos dizer o quão parecidos ou não são os objetos que desejamos agrupar. Para isto, medidas de distância são usadas para determinar tal similaridade, por exemplo, a distância euclidiana entre os pontos de dois objetos. Explicamos a diferença entre cientista e analista de dados no nosso minicurso de Introdução à análise de dados. Na Figura 4.2 vemos dois exemplos de agrupamentos em um espaço com duas dimensões (\\(X\\) e \\(Y\\)). O primeiro agrupamento 4.2(a) possui 3 grupos e o segundo 8 4.2(b). Note que o termo objeto refere-se a uma linha do conjunto de dados, conforme ilustrado, em que um objeto no Grupo 8 possui os valores de \\(X = 8\\) e \\(Y = 8\\). Figure 4.2: Exemplo de dois agrupamentos em um plano com duas dimensões (X,Y). Observam-se 3 grupos em a) e 8 grupos em b). Adaptado de Esling and Agon (2012) De acordo com Jain (2010), um dos grandes estudiosos da área de machine learning, o uso de técnicas agrupamento podem ser divididos em três principais aplicações: Descoberta de conhecimento em estruturas intrínsecas: para obter informações sobre dados, gerar hipóteses, detectar anomalías e identificar características salientes. Classificação natural: por exemplo, na Biologia, para identificar o grau de semelhança entre formas ou organismos (relação filogenética). Compressão: como um método para organizar os dados e resumi-los através de protótipos de agregados. Além das aplicações mencionadas por Jain (2010), como descoberta de padrões, detecção de outliers e análise de distribuições intrínsecas nos dados; segundo Han, Pei, and Kamber (2011), às técnicas de agrupamento podem ser usadas em etapas de pré-processamento para outros algoritmos, por exemplo, na caracterização, seleção de subconjuntos de atributos e classificação. Desta forma, diante dessas aplicações, o uso de técnicas de agrupamento encontra-se em diversas áreas do conhecimento, entre elas: Biologia: (Gasch and Eisen 2002), Sensoriamento Remoto: (He et al. 2014) e business intelligence: (Wang, Wu, and Zhang 2005). Agora que entendemos o que é um agrupamento e suas aplicações, na próxima subseção vamos mostrar algumas técnicas de agrupamento e quais as principais diferenças entre elas. 4.2 Técnicas de agrupamento Como vimos anteriormente, cada técnica de agrupamento possui uma abordagem para criar grupos em um conjunto de dados, por exemplo, algumas técnicas são baseadas em pontos centrais de cada grupo (centróide); em outras os grupos são criados seguindo uma hierarquia presente nos dados. Neste minicurso, vamos mostrar três abordagens de técnicas de agrupamento: baseadas em partição e hierarquia. A seguir vamos comentar de maneira bem resumida cada uma. 4.2.1 Técnicas baseadas em Partição Nas técnicas baseadas em partição, cada grupo representa uma partição, e o número de grupos é definido pelo usuário. Desta forma, cada partição deve conter pelo menos um objeto, e cada objeto deve pertencer somente a um grupo, o que é conhecido como separação exclusiva de grupos (em inglês, exclusive cluster separation ou hard cluster). Existem métodos que flexibilizam o critério de separação exclusiva, sendo assim, o objeto possui porcentagens de pertencimento a cada grupo, o que é conhecimento como técnicas de agrupamento nebulosas (em inglês, fuzzy clustering). A Figura 4.3 mostra um exemplo de agrupamento baseado em partição, as linhas pretas demarcam cada partição. Figure 4.3: Exemplo de um agrupamento beaseado em partição. As linhas pretas demarcam as partições e as figuras preenchidas os centróides. Adaptado de Developers (2020) Além das características mencionadas acima, grande parte das técnicas baseadas em partição usam medidas de distâncias para determinar o grau de similaridade de cada grupo. Outra característica importante de mencionar, é a representação de cada grupo, o centróide (Figura 4.3, que pode ser definido pela média ou qualquer outra medida estatística Um dos algoritmos mais utilizados baseados em partição é o Kmeans - vamos explicá-lo na subseção seguinte. 4.2.2 Técnicas baseadas em Hierarquia As técnicas baseadas em hierarquia, diferente dos que vimos nas técnicas de partição, não é requerido que o especialista especifique o número de grupos, pois seu algoritmo busca por correlações entre os objetos do conjunto de dados para criar os grupos. Os métodos hierárquicos podem ser divididos em aglomerativos ou divisivos, baseado no modo em que sua decomposição hierárquica é formada. No método aglomerativo, também conhecidos como AGNES (Agglomerative Nesting), considera-se, inicialmente, que cada objeto seja seu próprio grupo, e a cada iteração, os dois grupos mais parecidos são juntados, desta forma, formando um novo um grupo, esse processo é repetido até que todos os objetos estejam em um único grupo. De modo oposto, o método divisivo, também conhecidos como DIANA (Divise Analysis), considera-se que todos objetos pertencem a um único grupo, e posteriormente, divide-se o único grupo até obter-se um grupo para cada objeto ou até que um critério de párada seja atendido (Han, Pei, and Kamber 2011). Figure 4.4: Exemplo do processo de criação de grupos baseado nas técnicas de agrupamento hierarquicas. - Fonte: Han, Pei, and Kamber (2011) Na Figura 4.4 é possível observar o processo de criação de grupos dos dois métodos mencionados, AGNES e DIANA, com base em um conjunto de dados com cinco objetos \\(a, b, c, d, e\\). Por exemplo, no método aglomerativo, o objeto \\(a\\) junta-se com o objeto \\(b\\) no mesmo grupo, pois possuem maior semelhança se comparados com outros objetos, o mesmo processo acontece com os objetos \\(d\\) e \\(e\\). Por outro lado, o método de divisão inicia com todos os objetos pertencendo a um grupo, e a cada passo vai se dividindo com base na semelhança dos objetos dentro do grupo. Uma técnica que auxilia na interpretação dos agrupamentos hierárquicos é o dendrograma, apresentado na Figura 4.5, representando em forma de árvore, nele é possível obter informações sobre a estrutura do agrupamento realizado. No nível 0 é possível observar os objetos em seus próprios grupos, e, conforme o nível da árvore aumenta em relação a sua raiz, menor são as similaridades entre os objetos do mesmo grupo (Han, Pei, and Kamber 2011). Figure 4.5: Exemplo de representação do agrupamento hierarquico. - Fonte: Han, Pei, and Kamber (2011) Neste minicurso, vamos abordar o método aglomerativo, no entanto, os mesmos conceitos podem ser aplicados no método divisivo. 4.3 Kmeans Kmeans ou K-médias é uma técnica de agrupamento que usa o método de partição para dividir o conjunto de dados em \\(k\\) grupos, em que o valor de \\(k\\) é definido pelo usuário - posteriormente vamos apresentar algumas heurísticas que auxiliam na determinação do número de grupos. De forma geral, o algoritmo do Kmeans visa diminuir a variação intra-grupos, ou seja, criar grupos em que os objetos sejam semelhantes entre si. Então, para que esse objetivo seja atendido, a cada iteração os centros de cada grupo são atualizados para refinar a qualidade dos grupos. Para exemplificar o funcionamento do Kmeans, vamos usar o conjunto de dados apresentado na Figura 4.6, no qual é possível observar 13 objetos em um plano cartesiano (X,Y). Figure 4.6: Conjunto de dados utilizado como exemplo. Para separar as observações de forma a obter grupos mais homogêneos, a técnica em questão usa o conceito de centróide, em que é definido um ponto central para cada grupo com base em medidas estatísticas, como a média, e a cada iteração o ponto central é atualizado até que o critério de párada seja alcançado. Então, o primeiro passo do algoritmo é a escolha da quantidade de grupos, neste exemplo, escolhemos três grupos. Com isso, a próxima etapa consiste em sortear, de forma aleatória, o centróides de cada grupo, como apresentado na Figura 4.7. Após o sorteio, os objetos são atribuidos aos seus respectivos grupos, como também apresentado na Figura 4.7. A atribuição é feita com base em uma medida de distância, geralmente, euclidiana; cada objeto é comparado com cada centróide, assim, o objeto é atribuído ao grupo em que possui a menor distância com seu centróide. Figure 4.7: Exemplo de funcionamento do algoritmo Kmeans. Na Figura 4.7 observamos três centróides \\(c_1\\), \\(c_2\\) e \\(c_3\\), nas posições: \\((1.5,1.3)\\), \\((4.3,1.7)\\) e \\((2.7,3.3)\\), respectivamente. Para sabermos em qual grupo o objeto será atribuído basta usar uma medida de distância e verificar qual centróide está mais próximo deste objeto. Por exemplo, seja o objeto que está na posição \\((3.2,1)\\), temos: \\[ x_{obj} = 3.2\\\\ y_{obj} = 1.0\\\\ \\\\ x_{c1} = 1.5\\\\ y_{c1} = 1.3\\\\ \\\\ x_{c2} = 4.3\\\\ y_{c2} = 1.7\\\\ \\\\ x_{c3} = 2.7\\\\ y_{c3} = 3.3\\\\ \\\\ dist_{c1} = \\sqrt{(x_{obj} - x_{c1})^2 + (y_{obj} - y_{c1})^2} \\approx 1.73 \\\\ dist_{c2} = \\sqrt{(x_{obj} - x_{c2})^2 + (y_{obj} - y_{c2})^2} \\approx 1.30 \\\\ dist_{c3} = \\sqrt{(x_{obj} - x_{c3})^2 + (y_{obj} - y_{c3})^2} \\approx 2.35\\\\ \\] Desta forma, a menor distância (\\(dist\\)) é dada pelo centróide \\(c_2\\), então o objeto é atribuído ao grupo deste centróide. Após a atribuição, o centróide é recalculado com base nos objetos do grupo (Figura 4.8), como foi dito anteriormente, podemos usar medidas estatísticas, como a média para achar a nova posição do centróide. Caso, não haja alterações nos valores dos centróides, o algoritmo pára. Caso não, o processo se repete: atualização dos centróides e atribuição dos objetos a cada grupo. Figure 4.8: Exemplo de funcionamento do algoritmo Kmeans. Resumidamente, podemos descrever o algoritmo do Kmeans conforme as etapas abaixo: (1º Passo) Definição da quantidade de grupos; (2º Passo) Sorteio de centróides para cada grupo; (3º Passo) Atribuição dos objetos a cada grupo; (4º Passo) Atualização dos centróides de cada grupo; (5º Passo) Caso os centróides sejam atualizados, volte ao passo (3), caso não, o algoritmo pára. Lembrando que, outro critério de párada é o número de iterações, pois nem sempre é possível convergir para o ótimo local com poucas iterações. Ainda mais com conjuntos de dados de aplicações reais. Outro fato a mencionar, é o sorteio de centróides, pois existem diversas métricas que podem ser utilizadas em relação a inicialização dos centróides, por exemplo, k-means++. Agora que entendemos mais de perto como o kmeans funciona. Talvez, você esteja se perguntando,mas, como verificamos a eficiência do nosso agrupamento? Isso é o que vamos responder na próxima subseção. Também vamos comentar como podemos estimar o número de grupos em um conjunto de dados. 4.3.1 Como avaliar o Kmeans? Como mencionado anteriormente, o objetivo do kmeans é minimizar a variação intra-grupos, vimos que isso é feito com base na atualização dos centróides em cada iteração do algoritmo, até que não haja mudança na atualização dos centróides. Então, para verificarmos o erro de cada grupo, podemos usar a soma das diferenças quadráticas (do ingles, sum of squared error) entre cada objeto e o centróide do seu grupo. Por exemplo, na Figura 4.8 para calcularmos o erro quadrático do Grupo 1, temos a seguinte equação: \\[ SSE(grupo_{1}) = \\sum_{x_i \\in grupo_{1}} dist(x_i - c_1)^2 \\] Em que \\(x_i\\) representa cada objeto do grupo, \\(c_1\\) representa o centróide do grupo 1 e \\(dist\\) a medida de distância utilizada. Logo, o \\(SSE(grupo_1) \\approx 1.9\\), em outras palavras, o erro quadrático do grupo 1 é a distância de todos os objetos em relação seu centróide. Então, para calcularmos o erro total dos grupos, basta fazermos um somatório de cada SSE: \\[ SSE_{total} = SSE(grupo_1) + SSE(grupo_2) + SSE(grupo_3) \\] Então, para este agrupamento apresentado no exemplo acima, temos o \\(SSE_{total} \\approx x\\), em outras palavras, o erro total do nosso agrupamento, é representado pela soma dos erros quadráticos de cada grupo. Agora que vimos como calcular o erro do nosso agrupamento, só nos resta uma forma de “descobrir” qual a quantidade ideal de grupos para cada agrupamento e é que vamos ver na próxima subseção. 4.3.2 Como definir a quantidade de grupos? Nesta subseção vamos mostrar dois métodos para definir a quantidade de grupos, sendo eles: Método do Cotovelo e Método da Silhueta. É recomendado perguntar ao especialista sobre a quantidade de grupos, no caso da ausencia de um, podemos optar pelo uso de heurísticas. Duas deles são apresentadas abaixo. 4.3.2.1 Método do cotovelo O método do cotovelo (do inglês, elbow) usa o somatório do erro total dos grupos em cada agrupamento para determinar o número, como apresentado na Figura 4.9. Então, a ideia deste método é gerar diversos agrupamentos com diferentes números de grupos, com o incremento do número de grupos, o erro total tende a diminuir, até que o erro se aproxime de zero, que é quando terá um objeto por grupo. Figure 4.9: Método do cotovelo aplicado no conjunto de dados de exemplo. De acordo com o método, o número ideal de grupos se dá quando o ponto forma uma curva semelhante de um cotovelo, que é o ponto considerado ideal. Pois os pontos acima do cotovelo estão em uma região em que underfitting e abaixo dele em overfitting. 4.3.2.2 Método de Silhueta O método de Silhueta determina o quão bem cada objeto está alocado em um grupo, ou seja, a homogeneidade de um grupo. O índice de Silhueta varia de -1 a 1. Valores próximos a 1 indicam que o objeto possui semelhança com objetos de seu grupo e dessemelhança com objetos de outros grupos. Figure 4.10: Método de Silhueta aplicado no conjunto de dados de exemplo. A Figura 4.10 mostra um exemplo de aplicação do método de Silhueta, o número ideal de grupos é escolhido com base na média do maior do índice de Silhueta, ou seja, com 3 grupos foi possível obter grupos com objetos semelhantes entre si, e dissemelhante se comparados com objetos de outros grupos. Agora que entendemos os fundamentos da técnica de agrupamento Kmeans e algumas técnicas de estimativas de grupos, vamos apresentar alguns exemplos práticos na plataforma kaggle, os exemplos estão disponíveis nas linguagens R e Python. 4.4 Agrupamento Hierarquico - Método Aglomerativo Os métodos de agrupamento hierárquicos que pertencem à categoria de aglomerativos baseia na junção de grupos até que apenas um grupo englobe todos os outros, esse chamado de raiz. Os objetos no método aglomerativo são comparadas com base em uma medida de distância, como vimos no Kmeans, em que foi usado a distância euclidiana. No entanto, neste método, os grupos também são comparados, para que a união entre eles também seja realizada. Para compararmos a similaridade entre cada grupo usamos as medidas de ligação (do inglês, linkage measures). As medidas de ligação mais conhecidos são: Ligação completa ou distância máxima: Em inglês, complete linkage, determina como medida de similaridade os pares mais distantes entre dois grupos. \\[dist_{max}(g_1, g_2) = \\underset{obj_{g_1} \\in g_1,obj_{g_2} \\in g_2}{\\max \\{|obj_1 - obj_2|\\}}\\] Em que \\(g_1\\) e \\(g_2\\) correspondem aos grupos 1 e 2, respectivamente. Ligação única ou distância mínima: Em inglês, single linkage, determina como medida de similaridade os para mais próximo entre dois grupos. \\[dist_{min}(g_1, g_2) = \\underset{obj_{g_1} \\in g_1,obj_{g_2} \\in g_2}{\\min \\{|obj_1 - obj_2|\\}}\\] Distância baseada em média: Em inglês, average method, determina a distância entre cada grupo com base na média de todos os pares de objetos. \\[dist_{avg} = \\frac{1}{|g_1||g_2|} \\sum_{i = 1}^{g_1}\\sum_{j=1}^{g_2}dist(obj_i, obj_j)\\] Para exemplificar o funcionamento do método aglomerativo, vamos usar o conjunto de dados apresentado na Figura 4.11, no qual é possível observar 6 objetos (A,B,C,E,F) em um plano cartesiano (X,Y). Os objetos foram marcados para facilitar a visualização do dendograma. Figure 4.11: Conjunto de dados utilizado como exemplo. Como foi mencionado, os métodos aglomerativos iniciam de forma em que cada objeto seja seu próprio grupo, e com isso, a cada iteração os grupos vão se juntando e combinando os objetos dentro deles. O primeiro passo do algoritmos hierárquicos, abrangendo às duas categorias, divisivos e aglomerativo, é calcular a matriz de distância entre todos os objetos. A Figura 4.12 mostra um exemplo do cálculo da distância do objeto A entre os outros. Figure 4.12: Conjunto de dados utilizado como exemplo. Vamos deixar como tarefa para que você calcule a distância euclidiana entre os outros objetos. Então, seguindo com o algoritmo, o próximo passo consiste na junção dos objetos em grupos, como foi dito, cada objeto inicialmente pertence ao seu próprio grupo. A etapa de junção dos objetos aos grupos segue a mesma ideia do que vimos no Kmeans, só que aqui estamos comparando dois objetos, e não os objetos com seus centróides. Então, os objetos mais próximos se juntam em um grupo, como apresentado na Figura 4.13. A junção segue uma ordem, os objetos com as menores distâncias se juntam primeiro, então, neste exemplo, primeiro vamos juntar os objetos E e F que estão em 1 unidade de distância, depois juntamos os objetos A e B, como mostrado na Figura 4.12, que possuem a distância de \\(1.41\\), e, por fim, juntamos os objetos D e C que possuem a distância de \\(1.5\\). Utilizando o Dendrograma é possível observar essas distância no eixo de Altura. Figure 4.13: Etapa de junção dos objetos. Depois de juntarmos todos os pares de objetos em um grupo, que até então estavam em seus próprios grupos, é hora de juntarmos os grupos. Nesta etapa de junção dos grupos, vamos utilizar as abordagens de ligação que foram mencionadas acima. A Figura 4.14 apresenta o funcionamento dos métodos de ligação mencionados, em A) temos a ligação completa, que visa juntar os dois grupos mais próximos pelos objetos mais distantes; em B) temos a ligação única, que faz a junção de modo oposto do que vimos em A), usando os objetos mais próximos de cada grupo, e em C), temos a ligação pela média, que visa realizar a junção por meio da média das distâncias entre cada objeto dos dois grupos. Figure 4.14: Etapa de junção dos grupos. Por fim, a Figura 4.15 apresenta o resultado final do agrupamento aglomerativo por meio do método de ligação completa, veja que ao final apenas um grupo engloba todo os objetos. Na junção do grupo que engloba os objetos A, B, C e D com o grupo dos objetos E e F, a distância máxima (4.16) se dá pelos objetos A e F. Vamos deixar como tarefa para que você monte os dendrogramas dos métodos de ligação única e por média. Lembrando que o dendrograma mostra a distância entre os grupos por meio do eixo de altura. Desta forma, o dendrograma varia com o método de ligação. Por exemplo, o dendrograma do método de ligação única é mais achatado, visto que a maior altura corresponde a 2.06. Figure 4.15: Etapa de junção dos grupos - Parte 2. Resumidamente, podemos descrever o algoritmo do método aglomerativo conforme as etapas abaixo: (1º Passo) Crie a matriz de distância entre os objetos; (2º Passo) Faça com que cada objeto seja um grupo; (3º Passo) Junte os grupos mais próximos; (4º Passo) Atualize a matriz de distância; (5º Passo) Repita o passo 3 e 4 até que apenas um grupo englobe todos os objetos. Agora que entendemos como funciona o método aglomerativo, você deve estar se perguntando: “Qual método de ligação devo usar?” e é isso é o que vamos ver agora. 4.4.1 Qual método de ligação deve ser usado? Assim como a escolha do número de grupos no Kmeans, a escolha do método de ligação também é essencial, no entanto, cabe ao usuário testar os diversos métodos de ligação para determinar qual se adequa melhor ao seu caso. Vamos listar algumas vantagens e desvantagens dos métodos menionados acima retirados do material dos professores Gao and Zhang (2012). Método Vantagens Desvantagens Ligação única Consegue lidar com formas não elípticas Sensível a ruídos e outliers Ligação Completa Consegue lidar melhor com ruídos e outliers Tende a quebrar grandes grupos; Possui viés para agrupamentos com dados circulares. Distância baseada em média Consegue lidar melhor com ruídos e outliers Possui viés para agrupamentos com dados circulares. Além dos métodos de ligação mencionados acima, existem outros que são igualmentes importantes, como: Ward e Centróide 4.4.2 Como definir a quantidade de grupos? Como vimos, o dendrograma mostra a hierarquia do nosso agrupamento com o formato de árvores, aos que estão familiarizados com estrutura de dados, especificamente uma árvore binária. Assim, podemos fazer um corte nessa árvore e selecionar os grupos que desejamos de acordo com a altura especificada. Por exemplo, a Figura 4.16 mostra a aplicação do método aglomerativo por meio de ligação completa no conjunto de dados utilizado no exemplo do Kmeans, os \\(K\\) em cada dendrograma é o nível em que o corte foi realizado. Desta forma, cortando um nível abaixo da raiz, ou seja, \\(k = 2\\), obteve-se dois grupos, com \\(k = 5\\), obteve-se cinco grupos, e assim por diante. O corte no dendrograma nos dá uma visão dos grupos intrínsecos em cada nó da árvore, o que pode auxiliar no entendimento do conjunto de dados Figure 4.16: Cortes nos dendrogramas para visualização dos grupos. Como mostrado acima, o \\(k\\) define a quantidade de grupos que serão mostrados no dendrograma através de um corte feito numa árvore binária. E como foi visto no Kmeans utilizamos as heurísticas do cotovelo e silhueta para determinar o número de \\(k\\), o que pode ser feito neste método também. Você deve estar se perguntando: “Como faço para avaliar os grupos gerados no corte?”. Para avaliarmos a qualidade dos grupos podemos usar o índice de Silhueta, que verifica o quão bem os objetos foram ajustados em um grupo e quão eles se diferenciam dos objetos de outros grupos. Agora que entendemos os fundamentos da técnica de agrupamento aglomerativa e como podemos definir os grupos, vamos apresentar alguns exemplos práticos na plataforma kaggle, os exemplos estão disponíveis nas linguagens R e Python. 4.4.3 Exemplos 4.5 Saiba mais "],
["considerações-finais.html", "5 Considerações finais", " 5 Considerações finais Lorem ipsum dolor sit amet "],
["referências-bibliográficas.html", "6 Referências Bibliográficas", " 6 Referências Bibliográficas "]
]
