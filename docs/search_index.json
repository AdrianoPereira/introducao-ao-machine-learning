[
["introdução.html", "Agrupamento 1 Introdução 1.1 Machine learning", " Agrupamento 1 Introdução “As máquinas podem pensar?” A pergunta acima faz parte de um exercício teórico proposto pelo cientista da computação Alan Turing em seu artigo publicado em 1950 (TURING 1950). Conhecido também como jogo da imitação, o teste de Turing constitui, em sua concepção inicial, na interação entre três agentes: um agente interrogador e dois agentes respondentes, onde um dos agentes repondentes é um ser humano e outro uma máquina (computador). A pergunta enviada pelo agente interrogador é recebida por ambos os agentes respondentes, onde cada um deles devem enviar de volta a resposta. Com base nas respostas, o agente interrogador deve determinar quem é o humano é que é a máquina, a partir do momento em que esse agente não consegue mais fazer essa diferenciação, é dito que a máquina passou no teste. A Figura 1.1 mostra o esquema básico desse teste. Figure 1.1: Esquema do teste de Turing clássico. Diversas derivações desse teste surgiram posteriormente, o mais famoso deles e familiar entre a maioria dos internautas é o CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart), mecanismo de seguraça proposto por Von Ahn et al. (2003) para validar requisições através da resolução de pequenos desafios, que podem ser identificação de imagens ou textos distorcidos e com ruídos, e que tem como propósito dificultar o acesso não convencional a formulários, por exemplo, tentar impedir o uso bots. O teste de Turing talvez tenha sido um ponto de partida para o que hoje conhecemos por aprendizado de máquina (ML - sigla do inglês, Machine Learning) . A possibilidade de representar pensamentos em computadores, similares aos dos seres vivos foi um grande marco na humanidade. Atualmente esse conceito está sendo aplicado nas mais diversas áreas, tendo em algumas tarefas, o desempenho superior ao dos seres humanos. O próprio CAPTCHA é um exemplo disso, em algumas de suas versões iniciais o conteúdo ficava tão distorcido, que acabava dificultando a sua identificação pelos humanos, em contrapartida, os algoritmos conseguiam resolver o desafio com certa facilidade. Neste capítulo, será apresentada uma visão geral sobre o Machine Learning, discorrendo sobre as principais classes de algoritmos e aplicações com ênfase na área espacial. Ao final deste capítulo o leitor deverá ser capaz de: Compreender o contexto histórico e a definição do ML; Diferenciar as principais abordagens de treinamento dos modelos de ML; Diferenciar as principais classes de algoritmos de ML; Compreender as etapas mínimas necessárias para a produção de um modelo de ML; 1.1 Machine learning O ML é composto por uma coleção de métodos criados a partir de modelos matemáticos baseados na teoria da estatística que permitem aos computadores automatizarem tarefas com base na descoberta sistemática de padrões nos conjuntos de dados disponíveis ou em experiências passadas (Bhavsar et al. 2017; Alpaydin 2020). Segundo a definição de Samuel (1959), um dos pioneiros do assunto, o aprendizado de máquina é “um campo de estudo que oferece aos computadores a capacidade de aprender sem serem explicitamente programados”. "],
["classificação.html", "2 Classificação", " 2 Classificação Lorem ipsum dolor sit amet library(reticulate) ## ## Attaching package: &#39;reticulate&#39; ## The following object is masked from &#39;package:renv&#39;: ## ## use_python use_python(&quot;/home/adriano/anaconda3/bin/python&quot;) "],
["regressão.html", "3 Regressão 3.1 Regressão Linear", " 3 Regressão Todas as pessoas pelo menos uma vez na vida já sentiu ou sentirá a necessidade de prever algum acontecimento futuro. Estamos a todo momento assimilando informações para realizar alguma tomada de decisão, seja de forma intrínseca ou não. No contexto de Machine Learning (ML) isso é feito pela técnicas de regressão. A regressão é uma ferramenta que busca modelar relações entre variáveis dependentes e independentes através de métodos estatísticos (Soto 2013). Uma variável independente, normalmente representada pela variável \\(x\\), caracteriza uma grandeza que está sendo manipulada durante um experimento e que não sofre influência de outras variáveis. Já a variável dependente, normalmente representada pela variável \\(y\\), caracteriza valores que estão diretamente associados à variável independente, ou seja, ao ser manipulada os valores variável independente, o valor das variáveis dependentes também sofrem alterações. Na Figura 3.1 é apresentada a relação entre a expectativa de vida baseada e um índice de felicidade calculado em diversos países obtidos a partir de um levantamento feito por Helliwell et al. (2020). A variável independente nesse exemplo é representada pelo índice de felicidade e a expectativa de vida age como variável independente, dessa forma pode ser observada uma tendência de expectativa de vida maior em países com alto índice de felicidade, com uma força de correlação de 0,77. Figure 3.1: Relação entre o índice de felicidade e expectativa de vida. Fonte: (Helliwell et al. 2020) As relações entre as variáveis dependentes e independetes são feitas através de algum coeficiente de correlação. Uma das métricas de correlação mais utilizadas é o coeficiente de Pearson, que mede a associação linear entre duas variáveis (Kirch 2008). Os valores do coeficiente de Pearson variam entre -1 e 1, de tal forma que quanto mais próximos desses extremos, melhor correlacionado estão as variáveis. A Figura 3.2 mostra alguns exemplos com gráficos de disperssão de variáveis com diferentes correlações. Figure 3.2: Diferentes correlações entre variáveis. Fonte: (Helliwell et al. 2020) Os métodos de regressão se utilizam dessas correlações entre as variáveis para estimar valores não existentes na amostra ou conjunto de dados. Entretanto, nem sempre essas correlações são tão explicítas assim, sendo necessário outras abordagens mais robustas para realizar as previsões.Em ML os modelos de regressão podem ser criados a partir de diversas abordagens, desde as mais simples com poucas configurações de parâmetros e de fácil interpretação do funcionamento, até as abordagens mais complexas. Os métodos de regressão abordados neste capítulo serão Regressão linear, Máquina de vetores de suporte e Árvores de decisão. 3.1 Regressão Linear A regressão linear é um dos métodos mais intuitivos e utilizados para essa finalidade. Esses métodos são dividos em dois grupos, a regressão linear simples (RLS) e regressão linear múltipla (RLM). A RLS tem como objetivo estabelecer uma relação entre duas variáveis através de uma função, que pode ser definida por: \\[\\begin{equation} y_{i} = \\alpha+\\beta x_{i} \\tag{3.1} \\end{equation}\\] Onde \\(y_{i}\\) é a variável alvo, \\(\\alpha\\) e \\(beta x_{i}\\) são coeficientes calculados pela regressão, que representam o intercepto no eixo \\(y\\) e inclinação da reta, respectivamente. A RLM é semelhante semelhante à RLS, porém possui multiplas variáveis preditoras, e pode ser definida por: \\[\\begin{equation} y_{i} = \\alpha+\\beta x_{i1}+\\beta x_{i2}+...+\\beta x_{in} \\tag{3.2} \\end{equation}\\] Onde \\(y_{i}\\) é a variável alvo, \\(\\alpha\\) continua sendo o coeficiente de intercepto e \\(\\beta x_{ip}\\) o é coeficiente angular da \\(p\\)-ésima variável. Ambos os métodos podem ainda serem somados a um termo \\(\\epsilon\\) de erro. 3.1.1 Coeficientes da regressão linear Existem diversas abordagens para se calcular os coeficientes \\(\\alpha\\) e \\(\\beta\\) da equação da regressão linear, as técnicas baseadas em mínimos quadrados oridinários e gradiente descendente são as mais comuns. A seguir serão apresentados os funcionamentos dessas abordagens. 3.1.1.1 Métodos dos quadrados ordinários O Método dos quadrados ordinários (MQO) ou método dos mínimos quadrados (MMQ), busca encontrar o melhor valor para os coeficientes citados anteriormente, de tal forma que a diferença absoluta entre o valor real e o predito pela função seja a menor possível entre todos os pontos. A Figura 3.3 mostra um exmplo de regressão linar utilizando o MQO para o conjunto pontos descritos na tabela a seguir: | Variável independente | Variável dependente | | 0.44 | 5.52 | | 1.74 | 8.89 | | 0.41 | 4.05 | | 1.84 | 9.31 | | 0.98 | 6.57 | | 1.22 | 8.27 | | 1.53 | 6.93 | | 1.04 | 6.41 | | 0.59 | 6.93 | | 0.38 | 6.98 | Figure 3.3: Exemplo do método dos quadrados ordinários. Para se chegar no resultado apresentado na Figura 3.3, os coeficientes da regressão linear foram ajustados, de tal tal forma que o erro quadrático médio entre entre a função e cada um dos pontos fossem a menor possível. A Figura 3.4 mostra o ajuste dos coeficientes da equação em relação a cada ponto. Figure 3.4: Ajuste da regressão linear por método dos quadrados ordinários. 3.1.1.2 Gradiente descendente O método do gradiente descendente (GD) é uma das técnicas mais utilizadas para otimização de modelos de ML. Este é um método interativo que busca encontrar os coeficiente \\(\\alpha\\) e \\(\\beta\\) através da minimização de uma função de custo, que normalmente é o erro quadrático médio (MSE - sigla do inglês, mean squared error). O GD funciona de forma iterativa e inicializa os coeficientes com um valor predefinido ou aleatório. Em cada iteração é obtido o somatório do erro entre todos os valores das variáveis dependentes e valores calculados pela função. Com base nesse erro e em uma taxa de aprendizagem do modelo predefinida, os valores dos coeficientes da função são atualizados para a próxima iteração. A taxa de aprendizagem deve ser definda com um valor equilibrado. A definição de um valor muito alto para a taxa de aprendizagem pode levar o modelo a cair em um mínimo local, ou seja, o modelo não consegue chegar em seu melhor ajuste. Já quando a taxa de aprendizagem é definida com um valor muito baixo, o modelo demora mais tempo para chegar no ajuste ideal, necessitando de muito mais tempo e processamento até que haja a convergência. A Figura 3.5 mostra o comportamento do GD com diferentes categorias de valores mencionadas para a taxa de aprendizagem. Figure 3.5: Problemas na taxa de aprendizado do gradiente descendente. Os prinicipais parâmetros a serem definidos nessa abordagem são a taxa de aprendizagem e o número de iterações. Considerando os pontos utilizados no exemplo anterior, foi aplicada a regressão linear utilizando o GD como método de atualização dos coeficientes. A Figura 3.6 mostra o ajuste da função, custo e os coeficientes \\(\\alpha\\) e \\(\\beta\\) ao longo de 50 iterações com taxa de aprendizado muito baixa. Nessa figura pode ser observado que as iterações finalizam antes da convergência do modelo. Figure 3.6: Regressão linear com taxa de aprendizagem baixa no gradiente descendente. Como mencionado anteriormente, uma taxa de aprendizagem muito grande também interfere no ajuste dos coeficientes, uma vez o modelo não consegue atingir o mínimo global. A Figura 3.7 mostra o resultado da execução da regressão linear utilizando uma taxa de aprendizagem muito grande no GD. Figure 3.7: Regressão linear com taxa de aprendizagem baixa no gradiente descendente. Já com uma taxa de aprendizagem equilibrada, o GD é capaz de ajustar os coeficientes de forma mais eficiente. A Figura 3.8 mostra o resultado do algoritmo executado com uma taxa de aprendizagem mais equilibrada. Como os valores iniciais dos coeficientes são definidos de forma aleatória, nas primeiras iterações o gradiente apresenta uma alta pertubação, que vai se atenuando ao longo das épocas. Figure 3.8: Regressão linear com taxa de aprendizagem baixa no gradiente descendente. Para dados com poucas dimensões, ou seja, poucas variáveis preditoras, o MQO é mais recomendado, pois diferente do GD, não é um algoritmo interativo, e sua complexidade está associada diretamente à quantidade de pontos. Já o GD tem melhor performance quando os dados possuem muitas dimensões. A regressão linear pode ser aplicada em uma vasta variedade de problemas, mas como foi apresentado ao longo desta seção, é necessário que os dados possuam uma alta correlação. Este algoritmo está disponível na biblioteca Scikit-learn par ser utilizado em Python. Um exemplo utilizando a regressão linear é apresentado no código a seguir: from sklearn.linear_model import LinearRegression import matplotlib.pyplot as plt # Definição dos valores de uma única variável preditora x = [[0], [1], [2], [3]] # Definição dos valores das variáveis alvo y = [1.3, 2.2, 2.6, 3.3] # Instanciando regressão linear e ajustando-a aos dados lr = LinearRegression().fit(x, y) # Obtendo os valores aproximados da variável alvo y_regression = [lr.predict([xi])[0] for xi in x] # Criando novos pontos somente com valores da variável preditora x_test = [[-1], [2.75], [4.3]] # Aplicando regressão linear para prever os valores da variável alvo y_test = lr.predict(x_test) # Concatenando dados de treino e teste y_regression = [*y_regression, *y_test] x_regression = [*x, *x_test] # Exibindo dados de treino plt.scatter(x, y, color=&#39;gray&#39;, label=&#39;Dados de treino&#39;) # Exibindo dados de teste plt.scatter(x_test, y_test, color=&#39;red&#39;, label=&#39;Dados de teste&#39;) # Exibindo reta da regressão linear plt.plot(x_regression, y_regression, color=&#39;black&#39;, label=&#39;Regressão linear&#39;) #Configurações do gráfico plt.xlabel(&#39;Variável preditora ($x$)&#39;) plt.ylabel(&#39;Variável alvo ($y$)&#39;) plt.legend() plt.show() "],
["agrupamento.html", "4 Agrupamento", " 4 Agrupamento Lorem ipsum dolor sit amet "],
["considerações-finais.html", "5 Considerações finais", " 5 Considerações finais Lorem ipsum dolor sit amet "],
["referências-bibliográficas.html", "6 Referências Bibliográficas", " 6 Referências Bibliográficas Alpaydin, Ethem. 2020. Introduction to Machine Learning. MIT press. Bhavsar, Parth, Ilya Safro, Nidhal Bouaynaya, Robi Polikar, and Dimah Dera. 2017. “Machine Learning in Transportation Data Analytics.” In Data Analytics for Intelligent Transportation Systems, 283–307. Elsevier. Helliwell, John F, Haifang Huang, Shun Wang, and Max Norton. 2020. “Social Environments for World Happiness.” World Happiness Report 2020. Kirch, Wilhelm, ed. 2008. “Pearson’s Correlation Coefficient.” In Encyclopedia of Public Health, 1090–1. Dordrecht: Springer Netherlands. https://doi.org/10.1007/978-1-4020-5614-7_2569. Samuel, Arthur L. 1959. “Some Studies in Machine Learning Using the Game of Checkers.” IBM Journal of Research and Development 3 (3): 210–29. Soto, Timothy. 2013. “Regression Analysis.” In Encyclopedia of Autism Spectrum Disorders, edited by Fred R. Volkmar, 2538–8. New York, NY: Springer New York. TURING, A. M. 1950. “COMPUTING MACHINERY AND INTELLIGENCE.” Mind LIX (236): 433–60. https://doi.org/10.1093/mind/LIX.236.433. Von Ahn, Luis, Manuel Blum, Nicholas J Hopper, and John Langford. 2003. “CAPTCHA: Using Hard Ai Problems for Security.” In International Conference on the Theory and Applications of Cryptographic Techniques, 294–311. Springer. "]
]
