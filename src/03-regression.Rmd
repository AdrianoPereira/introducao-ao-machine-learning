---
output: bookdown::html_document2
bibliography: bibfile.bib
fig_caption: yes
header-includes:
- \usepackage{float}
- \floatsetup[table]{capposition=bot}
---

# Regressão
Todas as pessoas pelo menos uma vez na vida já sentiu ou sentirá a necessidade 
de prever algum acontecimento futuro. Estamos a todo momento assimilando 
informações para realizar alguma tomada de decisão, seja de forma intrínseca ou 
não. No contexto de Machine Learning (ML) isso é feito pela técnicas de 
regressão. A regressão é uma ferramenta que busca modelar relações entre 
variáveis dependentes e independentes através de métodos estatísticos 
[@soto2013regression]. 

Uma variável independente, normalmente representada pela  variável $x$, 
caracteriza uma grandeza que está sendo manipulada durante um experimento e que 
não sofre influência de outras variáveis. Já a variável dependente, normalmente 
representada pela variável $y$, caracteriza valores que estão diretamente 
associados à variável independente, ou seja, ao ser manipulada os valores 
variável independente, o valor das variáveis dependentes também sofrem 
alterações. Na Figura \@ref(fig:happinessWorld) é apresentada a relação entre a 
expectativa de vida baseada e um índice de felicidade calculado em diversos 
países obtidos a partir de um levantamento feito por @helliwell2020social. A 
variável independente nesse exemplo é representada pelo índice de felicidade e a 
expectativa de vida age como variável independente, dessa forma pode ser 
observada uma tendência de expectativa de vida maior em países com alto índice 
de felicidade, com uma força de correlação de 0,77. 

```{r happinessWorld, echo=FALSE, fig.align="center", fig.cap='Relação entre o índice de felicidade e expectativa de vida. Fonte: [@helliwell2020social]'}
  knitr::include_graphics("assets/happiness_world.png")
```

As relações entre as variáveis dependentes e independetes são feitas através de 
algum coeficiente de correlação. Uma das métricas de correlação mais utilizadas 
é o coeficiente de Pearson, que mede a associação linear entre duas variáveis 
[@kirck2008pearson]. Os valores do coeficiente de Pearson variam entre -1 e 1, 
de tal forma que quanto mais próximos desses extremos, melhor correlacionado 
estão as variáveis. A Figura \@ref(fig:scatterCorrelations) mostra alguns 
exemplos com gráficos de disperssão de variáveis com diferentes correlações.

```{r scatterCorrelations, echo=FALSE, fig.align="center", fig.cap='Diferentes correlações entre variáveis. Fonte: [@helliwell2020social]'}
  knitr::include_graphics("assets/correlations.png")
```

Os métodos de regressão se utilizam dessas correlações entre as variáveis para 
estimar valores não existentes na amostra ou conjunto de dados. Entretanto, nem
sempre essas correlações são tão explicítas assim, sendo necessário outras 
abordagens mais robustas para realizar as previsões.Em ML os modelos de 
regressão podem ser criados a partir de diversas abordagens, desde as mais 
simples com poucas configurações de parâmetros e de fácil interpretação do 
funcionamento, até as abordagens mais complexas. Os métodos de regressão 
abordados neste capítulo serão `Regressão linear`, 
`Máquina de vetores de suporte` e `Árvores de decisão`.

## Regressão Linear

A regressão linear é um dos métodos mais intuitivos e utilizados para essa 
finalidade. Esses métodos são dividos em dois grupos, a regressão linear simples
(RLS) e regressão linear múltipla (RLM). A RLS tem como objetivo estabelecer uma
relação entre duas variáveis através de uma função, que pode ser definida por:

\begin{equation} 
    y_{i} = \alpha+\beta x_{i}
(\#eq:rls-function)
\end{equation} 

Onde $y_{i}$ é a variável alvo, $\alpha$ e $beta x_{i}$ são coeficientes 
calculados pela regressão, que representam o intercepto no eixo $y$ e inclinação 
da reta, respectivamente.

A RLM é semelhante semelhante à RLS, porém possui multiplas variáveis 
preditoras, e pode ser definida por:

\begin{equation} 
    y_{i} = \alpha+\beta x_{i1}+\beta x_{i2}+...+\beta x_{in}
(\#eq:rlm-function)
\end{equation} 

Onde $y_{i}$ é a variável alvo, $\alpha$ continua sendo o coeficiente de 
intercepto e $\beta x_{ip}$ o é coeficiente angular da $p$-ésima variável. Ambos 
os métodos podem ainda serem somados a um termo $\epsilon$ de erro. 

### Coeficientes da regressão linear

Existem diversas abordagens para se calcular os coeficientes $\alpha$ e $\beta$ 
da equação da regressão linear, as técnicas baseadas em mínimos quadrados 
oridinários e gradiente descendente são as mais comuns. A seguir serão 
apresentados os funcionamentos dessas abordagens.

#### Métodos dos quadrados ordinários

O Método dos quadrados ordinários (MQO) ou método dos mínimos quadrados (MMQ),
busca encontrar o melhor valor para os coeficientes citados anteriormente, de 
tal forma que a diferença absoluta entre o valor real e o predito pela função 
seja a menor possível entre todos os pontos. A Figura \@ref(fig:ols) mostra um 
exmplo de regressão linar utilizando o MQO para o conjunto pontos descritos na
tabela a seguir:

|	Variável independente	|	Variável dependente	|
|	0.44	|	5.52	|
|	1.74	|	8.89	|
|	0.41	|	4.05	|
|	1.84	|	9.31	|
|	0.98	|	6.57	|
|	1.22	|	8.27	|
|	1.53	|	6.93	|
|	1.04	|	6.41	|
|	0.59	|	6.93	|
|	0.38	|	6.98	|

```{r ols, echo=FALSE, fig.align="center", fig.cap='Exemplo do método dos quadrados ordinários.'}
  knitr::include_graphics("assets/ols.png")
```

Para se chegar no resultado apresentado na Figura \@ref(fig:ols), os 
coeficientes da regressão linear foram ajustados, de tal tal forma que o erro 
quadrático médio entre entre a função e cada um dos pontos fossem a menor 
possível. A Figura \@ref(fig:ols-steps) mostra o ajuste dos coeficientes da 
equação em relação a cada ponto.

```{r ols-steps, echo=FALSE, fig.align="center", fig.cap='Ajuste da regressão linear por método dos quadrados ordinários.'}
  knitr::include_graphics("assets/ols-steps.png")
```

#### Gradiente descendente
O método do gradiente descendente (GD) é uma das técnicas mais utilizadas para 
otimização de modelos de ML. Este é um método interativo que busca encontrar os 
coeficiente $\alpha$ e $\beta$ através da minimização de uma função de custo, 
que normalmente é o erro quadrático médio (MSE - sigla do inglês, 
*mean squared error*).

O GD funciona de forma iterativa e inicializa os coeficientes com um valor 
predefinido ou aleatório. Em cada iteração é obtido o somatório do erro entre 
todos os valores das variáveis dependentes e valores calculados pela função. Com
base nesse erro e em uma taxa de aprendizagem do modelo predefinida, os valores
dos coeficientes da função são atualizados para a próxima iteração. A taxa de 
aprendizagem deve ser definda com um valor equilibrado. A definição de um valor 
muito alto para a taxa de aprendizagem pode levar o modelo a cair em um mínimo 
local, ou seja, o modelo não consegue chegar em seu melhor ajuste. Já quando a 
taxa de aprendizagem é definida com um valor muito baixo, o modelo demora mais 
tempo para chegar no ajuste ideal, necessitando de muito mais tempo e 
processamento até que haja a convergência. A Figura \@ref(fig:learning-rate) 
mostra o comportamento do GD com diferentes categorias de valores mencionadas 
para a taxa de aprendizagem.

```{r learning-rate, echo=FALSE, fig.align="center", fig.cap='Problemas na taxa de aprendizado do gradiente descendente.'}
  knitr::include_graphics("assets/learning-rate-gd.png")
```





 





