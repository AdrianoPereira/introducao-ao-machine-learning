---
output: bookdown::html_document2
bibliography: bibfile.bib
fig_caption: yes
header-includes:
- \usepackage{float}
- \floatsetup[table]{capposition=bot}
---

# Regressão
Todas as pessoas pelo menos uma vez na vida já sentiu ou sentirá a necessidade 
de prever algum acontecimento futuro. Estamos a todo momento assimilando 
informações para realizar alguma tomada de decisão, seja de forma intrínseca ou 
não. No contexto de Machine Learning (ML) isso é feito pela técnicas de 
regressão. A regressão é uma ferramenta que busca modelar relações entre 
variáveis dependentes e independentes através de métodos estatísticos 
[@soto2013regression]. 

Uma variável independente, normalmente representada pela  variável $x$, 
caracteriza uma grandeza que está sendo manipulada durante um experimento e que 
não sofre influência de outras variáveis. Já a variável dependente, normalmente 
representada pela variável $y$, caracteriza valores que estão diretamente 
associados à variável independente, ou seja, ao ser manipulada os valores 
variável independente, o valor das variáveis dependentes também sofrem 
alterações. Na Figura \@ref(fig:happinessWorld) é apresentada a relação entre a 
expectativa de vida baseada e um índice de felicidade calculado em diversos 
países obtidos a partir de um levantamento feito por @helliwell2020social. A 
variável independente nesse exemplo é representada pelo índice de felicidade e a 
expectativa de vida age como variável independente, dessa forma pode ser 
observada uma tendência de expectativa de vida maior em países com alto índice 
de felicidade, com uma força de correlação de 0,77. 

```{r happinessWorld, echo=FALSE, fig.align="center", fig.cap='Relação entre o índice de felicidade e expectativa de vida. Fonte: [@helliwell2020social]'}
  knitr::include_graphics("assets/happiness_world.png")
```

As relações entre as variáveis dependentes e independetes são feitas através de 
algum coeficiente de correlação. Uma das métricas de correlação mais utilizadas 
é o coeficiente de Pearson, que mede a associação linear entre duas variáveis 
[@kirck2008pearson]. Os valores do coeficiente de Pearson variam entre -1 e 1, 
de tal forma que quanto mais próximos desses extremos, melhor correlacionado 
estão as variáveis. A Figura \@ref(fig:scatterCorrelations) mostra alguns 
exemplos com gráficos de disperssão de variáveis com diferentes correlações.

```{r scatterCorrelations, echo=FALSE, fig.align="center", fig.cap='Diferentes correlações entre variáveis. Fonte: [@helliwell2020social]'}
  knitr::include_graphics("assets/correlations.png")
```

Os métodos de regressão se utilizam dessas correlações entre as variáveis para 
estimar valores não existentes na amostra ou conjunto de dados. Entretanto, nem
sempre essas correlações são tão explicítas assim, sendo necessário outras 
abordagens mais robustas para realizar as previsões.Em ML os modelos de 
regressão podem ser criados a partir de diversas abordagens, desde as mais 
simples com poucas configurações de parâmetros e de fácil interpretação do 
funcionamento, até as abordagens mais complexas. Os métodos de regressão 
abordados neste capítulo serão `Regressão linear`, 
`Máquina de vetores de suporte` e `Árvores de decisão`.

## Regressão Linear

A regressão linear é um dos métodos mais intuitivos e utilizados para essa 
finalidade. Esses métodos são dividos em dois grupos, a regressão linear simples
(RLS) e regressão linear múltipla (RLM). A RLS tem como objetivo estabelecer uma
relação entre duas variáveis através de uma função, que pode ser definida por:

\begin{equation} 
    y_{i} = \alpha+\beta x_{i}
(\#eq:rls-function)
\end{equation} 

Onde $y_{i}$ é a variável dependente, também denominada de variável alvo,
$\alpha$ e $beta x_{i}$ são coeficientes calculados pela regressão, que 
representam o intercepto no eixo $y$ e inclinação da reta, respectivamente.

A RLM é semelhante semelhante à RLS, porém possui multiplas variáveis 
preditoras, e pode ser definida por:

\begin{equation} 
    y_{i} = \alpha+\beta x_{i1}+\beta x_{i2}+...+\beta x_{in}
(\#eq:rlm-function)
\end{equation} 

Onde $y_{i}$ é a variável dependente, $\alpha$ continua sendo o coeficiente de 
intercepto e $\beta x_{ip}$ o é coeficiente angular da $p$-ésima variável. Ambos 
os métodos podem ainda serem somados a um termo $\epsilon$ de erro. 

### Cálculo dos coeficientes

Existem diversas abordagens para se calcular os coeficientes de intercepto de 
inclinação da reta, as técnicas baseadas em mínimos quadrados oridinários e 
gradiente descendente são as mais comuns.

#### Métodos dos quadrados ordinários

O Método dos quadrados ordinários (MQO) ou método dos mínimos quadrados (MMQ),
busca encontrar o melhor valor para os coeficientes citados anteriormente, de 
tal forma que a diferença absoluta entre o valor real e o predito pela função 
seja a menor possível entre todos os pontos. A Figura \@ref(fig:ols) mostra um 
exmplo de regressão linar utilizando o MQO para um conjunto de pontos com força 
de correlação de 0,9.

```{r ols, echo=FALSE, fig.align="center", fig.cap='Exemplo do método dos quadrados ordinários.'}
  knitr::include_graphics("assets/ols.png")
```

Considerando a Imagem \@ref(fig:ols), uma amostra de 10 pontos como exemplo, que
será chamada de $A$, ou seja, $A = \{(x_{i}, y_{i}):i=1, ..., 10\}$. O MQO tem 
como objetivo estibular os coeficientes da função, de tal forma que a soma 
quadrática dos erros absolutos $E^{2}$ seja a menor possível:

\begin{equation} 
    E^{2} = \sum_{i=1}^{n=10}(y_{i}-(\alpha+\beta x_{i}))^{2}
(\#eq:error-ols)
\end{equation} 

Expandindo a equação fica:

\begin{equation} 
    E = ((y_{1}-(\alpha+\beta x_{1}))^{2}+(y_{2}-(\alpha+\beta x_{2}))^{2}+ ... + (y_{10}-(\alpha+\beta x_{10}))^{2})^{2}
(\#eq:expanded-error-ols)
\end{equation} 

Após isso, é teremos uma função com duas variáveis, e para encontrar seus 
valores e igualar a zero, a fim de se obter os pontos críticos, é necessário 
calcular as derivadas parciais em relação a $\alpha$ (Equação 
\@ref(eq:partial-alpha) e $\beta$ (Equação \@ref(eq:partial-beta).

<!-- = 2(y_{1}-\alpha+\beta x_{1})\times1+2(y_{2}-\alpha+\beta x_{2})\times1+ ... + 2(y_{10}-\alpha+\beta x_{10})\times1 = 0 -->
\begin{equation} 
    \frac{\partial E^{2}}{\partial \alpha} = (y_{1}-(\alpha+\beta x_{1}))^{2}+(y_{2}-(\alpha+\beta x_{2}))^{2}+ ... + (y_{10}-(\alpha+\beta x_{10}))^{2} = 0
(\#eq:partial-alpha)
\end{equation} 

\begin{equation} 
    \frac{\partial E^{2}}{\partial \beta} = (y_{1}-(\alpha+\beta x_{1}))^{2}+(y_{2}-(\alpha+\beta x_{2}))^{2}+ ... + (y_{10}-(\alpha+\beta x_{10}))^{2} = 0
(\#eq:partial-beta)
\end{equation} 

Após calcular as derivadas parcias, teremos duas equações, das quais podemos 
encontrar os valores das duas variáveis utilizando um sistema de equações 
lineares. Após encontrar as variáveis e realizar as devidas substituições, 
teremos a equação da reta, ou seja, a regressão linear.

#### Gradiente descendente
O método do gradiente descendente (GD) é uma das técnicas mais utilizadas para 
otimização de modelos de ML. O GD é um método interativo que busca encontrar os 
coeficiente $\alpha$ e $\beta$ através da minimuzação de uma função de custo, 
normalmente o erro quadrático médio (MSE - sigla do inglês, 
*mean squared error*). O MSE pode ser definido pela Equação \@ref(eq:mse), onde
$e_{i}$ é a diferença entre o valor real de $y_{i}$ e o valor previsto pela 
equação.

\begin{equation} 
    MSE = \frac{1}{n} \sum^{n}_{i=1}e_{i}^{2}
(\#eq:mse)
\end{equation} 

O GD deve ajustar os coeficientes até que o MSE seja o menor possível e cada 
intereação. A atualização dos coeficiente é feita com base em uma taxa de 
aprendizado, normalmente definida em 0,0001. Uma taxa de aprendizado muito 
grande o GD pode cair em um mínimo local, já um valor muito baixo, o modelo pode 
demorar muito mais tempo para chegar em um mínimo local, necessitando de muito 
mais tempo e processamento, conforme ilustrado na Figura 
\@ref(fig:learning-rate).

```{r learning-rate, echo=FALSE, fig.align="center", fig.cap='Problemas na taxa de aprendizado do gradiente descendente.'}
  knitr::include_graphics("assets/learning-rate-gd.png")
```





 





